{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89eb4814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset: KarateClub():\n",
      "======================\n",
      "Number of graphs: 1\n",
      "Number of features: 34\n",
      "Number of classes: 4\n",
      "\n",
      "Data(x=[34, 34], edge_index=[2, 156], y=[34], train_mask=[34])\n",
      "======================\n",
      "Number of nodes: 34\n",
      "Number of edges: 156\n",
      "Average node degree: 4.59\n",
      "Number of training nodes: 4\n",
      "Training node label rate: 0.12\n",
      "Contains isolated nodes: False\n",
      "Contains self-loops: False\n",
      "Is undirected: True\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.datasets import NELL\n",
    "from torch_geometric.datasets import KarateClub\n",
    "from torch_geometric.transforms import NormalizeFeatures\n",
    "from torch_geometric.nn import GCNConv, SAGEConv, GATConv\n",
    "import torch\n",
    "\n",
    "dataset = KarateClub()\n",
    "\n",
    "print()\n",
    "print(f'Dataset: {dataset}:')\n",
    "print('======================')\n",
    "print(f'Number of graphs: {len(dataset)}')\n",
    "print(f'Number of features: {dataset.num_features}')\n",
    "print(f'Number of classes: {dataset.num_classes}')\n",
    "data = dataset[0]\n",
    "# data = dataset[0]  # Get the first graph object.\n",
    "\n",
    "print()\n",
    "print(data)\n",
    "print('======================')\n",
    "\n",
    "# Gather some statistics about the graph.\n",
    "print(f'Number of nodes: {data.num_nodes}')\n",
    "print(f'Number of edges: {data.num_edges}')\n",
    "print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
    "print(f'Number of training nodes: {data.train_mask.sum()}')\n",
    "print(f'Training node label rate: {int(data.train_mask.sum()) / data.num_nodes:.2f}')\n",
    "print(f'Contains isolated nodes: {data.has_isolated_nodes()}')\n",
    "print(f'Contains self-loops: {data.has_self_loops()}')\n",
    "print(f'Is undirected: {data.is_undirected()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee402573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[-3.3860, -3.5249, -3.4277,  ..., -3.5709, -3.5910, -3.5579],\n",
      "        [-3.4776, -3.5409, -3.4814,  ..., -3.5509, -3.6132, -3.5509],\n",
      "        [-3.4031, -3.4791, -3.3990,  ..., -3.5526, -3.6344, -3.5699],\n",
      "        ...,\n",
      "        [-3.4751, -3.5127, -3.4639,  ..., -3.5435, -3.6171, -3.5531],\n",
      "        [-3.4336, -3.4713, -3.4610,  ..., -3.6113, -3.6030, -3.5439],\n",
      "        [-3.4483, -3.4836, -3.4177,  ..., -3.6024, -3.5996, -3.5670]],\n",
      "       grad_fn=<LogSoftmaxBackward>), tensor([[ 0.1497,  0.0108,  0.1081,  ..., -0.0351, -0.0552, -0.0221],\n",
      "        [ 0.0554, -0.0078,  0.0517,  ..., -0.0178, -0.0801, -0.0178],\n",
      "        [ 0.1402,  0.0641,  0.1442,  ..., -0.0093, -0.0911, -0.0266],\n",
      "        ...,\n",
      "        [ 0.0523,  0.0147,  0.0635,  ..., -0.0160, -0.0897, -0.0256],\n",
      "        [ 0.1029,  0.0653,  0.0755,  ..., -0.0748, -0.0665, -0.0073],\n",
      "        [ 0.0918,  0.0564,  0.1223,  ..., -0.0624, -0.0596, -0.0270]],\n",
      "       grad_fn=<AddBackward0>))\n",
      "epoch: 0 loss: 3.4975829124450684\n",
      "epoch: 1 loss: 3.465601921081543\n",
      "epoch: 2 loss: 3.4335975646972656\n",
      "epoch: 3 loss: 3.401503086090088\n",
      "epoch: 4 loss: 3.3693857192993164\n",
      "epoch: 5 loss: 3.3370847702026367\n",
      "epoch: 6 loss: 3.304563045501709\n",
      "epoch: 7 loss: 3.2717766761779785\n",
      "epoch: 8 loss: 3.238741636276245\n",
      "epoch: 9 loss: 3.205339193344116\n",
      "epoch: 10 loss: 3.1715481281280518\n",
      "epoch: 11 loss: 3.1373093128204346\n",
      "epoch: 12 loss: 3.102581262588501\n",
      "epoch: 13 loss: 3.0673129558563232\n",
      "epoch: 14 loss: 3.031534194946289\n",
      "epoch: 15 loss: 2.995159149169922\n",
      "epoch: 16 loss: 2.9581778049468994\n",
      "epoch: 17 loss: 2.9205732345581055\n",
      "epoch: 18 loss: 2.8822829723358154\n",
      "epoch: 19 loss: 2.8432371616363525\n",
      "epoch: 20 loss: 2.8034791946411133\n",
      "epoch: 21 loss: 2.7629716396331787\n",
      "epoch: 22 loss: 2.721543073654175\n",
      "epoch: 23 loss: 2.6792659759521484\n",
      "epoch: 24 loss: 2.636174201965332\n",
      "epoch: 25 loss: 2.592268228530884\n",
      "epoch: 26 loss: 2.547548770904541\n",
      "epoch: 27 loss: 2.5020530223846436\n",
      "epoch: 28 loss: 2.455808401107788\n",
      "epoch: 29 loss: 2.40883731842041\n",
      "epoch: 30 loss: 2.361253023147583\n",
      "epoch: 31 loss: 2.312925100326538\n",
      "epoch: 32 loss: 2.2639517784118652\n",
      "epoch: 33 loss: 2.2144243717193604\n",
      "epoch: 34 loss: 2.1642985343933105\n",
      "epoch: 35 loss: 2.1136698722839355\n",
      "epoch: 36 loss: 2.062674045562744\n",
      "epoch: 37 loss: 2.0113799571990967\n",
      "epoch: 38 loss: 1.959816813468933\n",
      "epoch: 39 loss: 1.9080649614334106\n",
      "epoch: 40 loss: 1.8562722206115723\n",
      "epoch: 41 loss: 1.804574728012085\n",
      "epoch: 42 loss: 1.753016471862793\n",
      "epoch: 43 loss: 1.7018225193023682\n",
      "epoch: 44 loss: 1.65108323097229\n",
      "epoch: 45 loss: 1.6008672714233398\n",
      "epoch: 46 loss: 1.5513733625411987\n",
      "epoch: 47 loss: 1.5025697946548462\n",
      "epoch: 48 loss: 1.454580545425415\n",
      "epoch: 49 loss: 1.407567024230957\n",
      "epoch: 50 loss: 1.3615741729736328\n",
      "epoch: 51 loss: 1.3165810108184814\n",
      "epoch: 52 loss: 1.2726728916168213\n",
      "epoch: 53 loss: 1.229888677597046\n",
      "epoch: 54 loss: 1.1881966590881348\n",
      "epoch: 55 loss: 1.1476200819015503\n",
      "epoch: 56 loss: 1.1082621812820435\n",
      "epoch: 57 loss: 1.070096731185913\n",
      "epoch: 58 loss: 1.0331488847732544\n",
      "epoch: 59 loss: 0.9974054098129272\n",
      "epoch: 60 loss: 0.9628074169158936\n",
      "epoch: 61 loss: 0.9294017553329468\n",
      "epoch: 62 loss: 0.8971042633056641\n",
      "epoch: 63 loss: 0.8658371567726135\n",
      "epoch: 64 loss: 0.8356058597564697\n",
      "epoch: 65 loss: 0.8063767552375793\n",
      "epoch: 66 loss: 0.7781387567520142\n",
      "epoch: 67 loss: 0.7508427500724792\n",
      "epoch: 68 loss: 0.7244318723678589\n",
      "epoch: 69 loss: 0.6988837718963623\n",
      "epoch: 70 loss: 0.6741524934768677\n",
      "epoch: 71 loss: 0.65024733543396\n",
      "epoch: 72 loss: 0.6271238327026367\n",
      "epoch: 73 loss: 0.6047664880752563\n",
      "epoch: 74 loss: 0.5831685066223145\n",
      "epoch: 75 loss: 0.5622946619987488\n",
      "epoch: 76 loss: 0.5420887470245361\n",
      "epoch: 77 loss: 0.522588312625885\n",
      "epoch: 78 loss: 0.5037702322006226\n",
      "epoch: 79 loss: 0.48559117317199707\n",
      "epoch: 80 loss: 0.46799662709236145\n",
      "epoch: 81 loss: 0.4510261118412018\n",
      "epoch: 82 loss: 0.434640109539032\n",
      "epoch: 83 loss: 0.41885101795196533\n",
      "epoch: 84 loss: 0.40359872579574585\n",
      "epoch: 85 loss: 0.3888803720474243\n",
      "epoch: 86 loss: 0.37473154067993164\n",
      "epoch: 87 loss: 0.361122727394104\n",
      "epoch: 88 loss: 0.34800341725349426\n",
      "epoch: 89 loss: 0.3353758454322815\n",
      "epoch: 90 loss: 0.32329219579696655\n",
      "epoch: 91 loss: 0.311650812625885\n",
      "epoch: 92 loss: 0.3004538416862488\n",
      "epoch: 93 loss: 0.28969407081604004\n",
      "epoch: 94 loss: 0.27936217188835144\n",
      "epoch: 95 loss: 0.2694419026374817\n",
      "epoch: 96 loss: 0.259918212890625\n",
      "epoch: 97 loss: 0.2507619857788086\n",
      "epoch: 98 loss: 0.24197670817375183\n",
      "epoch: 99 loss: 0.23351092636585236\n",
      "epoch: 100 loss: 0.22538939118385315\n",
      "epoch: 101 loss: 0.2175939679145813\n",
      "epoch: 102 loss: 0.21011848747730255\n",
      "epoch: 103 loss: 0.20295850932598114\n",
      "epoch: 104 loss: 0.19609391689300537\n",
      "epoch: 105 loss: 0.18949712812900543\n",
      "epoch: 106 loss: 0.18314123153686523\n",
      "epoch: 107 loss: 0.17705711722373962\n",
      "epoch: 108 loss: 0.171214759349823\n",
      "epoch: 109 loss: 0.1656084954738617\n",
      "epoch: 110 loss: 0.16022151708602905\n",
      "epoch: 111 loss: 0.15505224466323853\n",
      "epoch: 112 loss: 0.15008240938186646\n",
      "epoch: 113 loss: 0.14531303942203522\n",
      "epoch: 114 loss: 0.14074043929576874\n",
      "epoch: 115 loss: 0.13636349141597748\n",
      "epoch: 116 loss: 0.1321498304605484\n",
      "epoch: 117 loss: 0.12810644507408142\n",
      "epoch: 118 loss: 0.12422728538513184\n",
      "epoch: 119 loss: 0.12049774825572968\n",
      "epoch: 120 loss: 0.11691342294216156\n",
      "epoch: 121 loss: 0.11347043514251709\n",
      "epoch: 122 loss: 0.11016666889190674\n",
      "epoch: 123 loss: 0.10698729753494263\n",
      "epoch: 124 loss: 0.10392504930496216\n",
      "epoch: 125 loss: 0.10098336637020111\n",
      "epoch: 126 loss: 0.09815751016139984\n",
      "epoch: 127 loss: 0.09543970972299576\n",
      "epoch: 128 loss: 0.09282443672418594\n",
      "epoch: 129 loss: 0.09030750393867493\n",
      "epoch: 130 loss: 0.0878894031047821\n",
      "epoch: 131 loss: 0.08555909991264343\n",
      "epoch: 132 loss: 0.08331166952848434\n",
      "epoch: 133 loss: 0.08114172518253326\n",
      "epoch: 134 loss: 0.07905388623476028\n",
      "epoch: 135 loss: 0.0770399197936058\n",
      "epoch: 136 loss: 0.07509638369083405\n",
      "epoch: 137 loss: 0.07321429252624512\n",
      "epoch: 138 loss: 0.07139740139245987\n",
      "epoch: 139 loss: 0.0696428194642067\n",
      "epoch: 140 loss: 0.06795041263103485\n",
      "epoch: 141 loss: 0.06631503999233246\n",
      "epoch: 142 loss: 0.06473048031330109\n",
      "epoch: 143 loss: 0.06319789588451385\n",
      "epoch: 144 loss: 0.061715420335531235\n",
      "epoch: 145 loss: 0.060283295810222626\n",
      "epoch: 146 loss: 0.05890056490898132\n",
      "epoch: 147 loss: 0.057558879256248474\n",
      "epoch: 148 loss: 0.05626092106103897\n",
      "epoch: 149 loss: 0.055006638169288635\n",
      "epoch: 150 loss: 0.0537891760468483\n",
      "epoch: 151 loss: 0.05261024087667465\n",
      "epoch: 152 loss: 0.05146808922290802\n",
      "epoch: 153 loss: 0.05035891756415367\n",
      "epoch: 154 loss: 0.049287255853414536\n",
      "epoch: 155 loss: 0.04824887216091156\n",
      "epoch: 156 loss: 0.047239549458026886\n",
      "epoch: 157 loss: 0.04625961184501648\n",
      "epoch: 158 loss: 0.04530853033065796\n",
      "epoch: 159 loss: 0.044386282563209534\n",
      "epoch: 160 loss: 0.04348914697766304\n",
      "epoch: 161 loss: 0.04261771962046623\n",
      "epoch: 162 loss: 0.04177026078104973\n",
      "epoch: 163 loss: 0.04094964265823364\n",
      "epoch: 164 loss: 0.0401516929268837\n",
      "epoch: 165 loss: 0.039375267922878265\n",
      "epoch: 166 loss: 0.03861958533525467\n",
      "epoch: 167 loss: 0.037885211408138275\n",
      "epoch: 168 loss: 0.037172384560108185\n",
      "epoch: 169 loss: 0.03647690266370773\n",
      "epoch: 170 loss: 0.03580065444111824\n",
      "epoch: 171 loss: 0.035143621265888214\n",
      "epoch: 172 loss: 0.034502699971199036\n",
      "epoch: 173 loss: 0.03388010710477829\n",
      "epoch: 174 loss: 0.033273085951805115\n",
      "epoch: 175 loss: 0.03268105909228325\n",
      "epoch: 176 loss: 0.03210575878620148\n",
      "epoch: 177 loss: 0.031545910984277725\n",
      "epoch: 178 loss: 0.03099813684821129\n",
      "epoch: 179 loss: 0.030464913696050644\n",
      "epoch: 180 loss: 0.02994554117321968\n",
      "epoch: 181 loss: 0.029439613223075867\n",
      "epoch: 182 loss: 0.02894694358110428\n",
      "epoch: 183 loss: 0.02846606820821762\n",
      "epoch: 184 loss: 0.027996443212032318\n",
      "epoch: 185 loss: 0.027538012713193893\n",
      "epoch: 186 loss: 0.02709062024950981\n",
      "epoch: 187 loss: 0.02665555849671364\n",
      "epoch: 188 loss: 0.026230989024043083\n",
      "epoch: 189 loss: 0.025815676897764206\n",
      "epoch: 190 loss: 0.02541067823767662\n",
      "epoch: 191 loss: 0.02501533553004265\n",
      "epoch: 192 loss: 0.024629343301057816\n",
      "epoch: 193 loss: 0.024253176525235176\n",
      "epoch: 194 loss: 0.023884691298007965\n",
      "epoch: 195 loss: 0.023526260629296303\n",
      "epoch: 196 loss: 0.02317614108324051\n",
      "epoch: 197 loss: 0.022833965718746185\n",
      "epoch: 198 loss: 0.02250000461935997\n",
      "epoch: 199 loss: 0.022173745557665825\n",
      "epoch: 200 loss: 0.021855628117918968\n",
      "epoch: 201 loss: 0.02154429256916046\n",
      "epoch: 202 loss: 0.021239716559648514\n",
      "epoch: 203 loss: 0.0209418423473835\n",
      "epoch: 204 loss: 0.020651470869779587\n",
      "epoch: 205 loss: 0.02036755532026291\n",
      "epoch: 206 loss: 0.020089805126190186\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 207 loss: 0.01981741562485695\n",
      "epoch: 208 loss: 0.0195519607514143\n",
      "epoch: 209 loss: 0.019292399287223816\n",
      "epoch: 210 loss: 0.019038066267967224\n",
      "epoch: 211 loss: 0.018789660185575485\n",
      "epoch: 212 loss: 0.018546780571341515\n",
      "epoch: 213 loss: 0.01830875501036644\n",
      "epoch: 214 loss: 0.018075473606586456\n",
      "epoch: 215 loss: 0.017847610637545586\n",
      "epoch: 216 loss: 0.0176241472363472\n",
      "epoch: 217 loss: 0.017405521124601364\n",
      "epoch: 218 loss: 0.0171914491802454\n",
      "epoch: 219 loss: 0.016981814056634903\n",
      "epoch: 220 loss: 0.016776148229837418\n",
      "epoch: 221 loss: 0.016574982553720474\n",
      "epoch: 222 loss: 0.016378644853830338\n",
      "epoch: 223 loss: 0.016185812652111053\n",
      "epoch: 224 loss: 0.015996525064110756\n",
      "epoch: 225 loss: 0.01581239327788353\n",
      "epoch: 226 loss: 0.01563112810254097\n",
      "epoch: 227 loss: 0.015453588217496872\n",
      "epoch: 228 loss: 0.01527950819581747\n",
      "epoch: 229 loss: 0.015108277089893818\n",
      "epoch: 230 loss: 0.014941154047846794\n",
      "epoch: 231 loss: 0.01477658748626709\n",
      "epoch: 232 loss: 0.014615724794566631\n",
      "epoch: 233 loss: 0.014458447694778442\n",
      "epoch: 234 loss: 0.014303319156169891\n",
      "epoch: 235 loss: 0.01415072288364172\n",
      "epoch: 236 loss: 0.014001363888382912\n",
      "epoch: 237 loss: 0.01385551132261753\n",
      "epoch: 238 loss: 0.013712134212255478\n",
      "epoch: 239 loss: 0.013571178540587425\n",
      "epoch: 240 loss: 0.013432643376290798\n",
      "epoch: 241 loss: 0.013297469355165958\n",
      "epoch: 242 loss: 0.01316463015973568\n",
      "epoch: 243 loss: 0.013034126721322536\n",
      "epoch: 244 loss: 0.012905459851026535\n",
      "epoch: 245 loss: 0.012778952717781067\n",
      "epoch: 246 loss: 0.012655342929065228\n",
      "epoch: 247 loss: 0.01253410242497921\n",
      "epoch: 248 loss: 0.012415435165166855\n",
      "epoch: 249 loss: 0.01229805126786232\n",
      "epoch: 250 loss: 0.012182389385998249\n",
      "epoch: 251 loss: 0.012069334276020527\n",
      "epoch: 252 loss: 0.011958475224673748\n",
      "epoch: 253 loss: 0.011849254369735718\n",
      "epoch: 254 loss: 0.011741610243916512\n",
      "epoch: 255 loss: 0.011636105366051197\n",
      "epoch: 256 loss: 0.011532444506883621\n",
      "epoch: 257 loss: 0.011430630460381508\n",
      "epoch: 258 loss: 0.011330455541610718\n",
      "epoch: 259 loss: 0.011232009157538414\n",
      "epoch: 260 loss: 0.011135409586131573\n",
      "epoch: 261 loss: 0.011040274053812027\n",
      "epoch: 262 loss: 0.010946603491902351\n",
      "epoch: 263 loss: 0.010854281485080719\n",
      "epoch: 264 loss: 0.010763600468635559\n",
      "epoch: 265 loss: 0.010674504563212395\n",
      "epoch: 266 loss: 0.010586461052298546\n",
      "epoch: 267 loss: 0.010500503703951836\n",
      "epoch: 268 loss: 0.010415070690214634\n",
      "epoch: 269 loss: 0.010331281460821629\n",
      "epoch: 270 loss: 0.010248754173517227\n",
      "epoch: 271 loss: 0.010167958214879036\n",
      "epoch: 272 loss: 0.010088278912007809\n",
      "epoch: 273 loss: 0.01000924315303564\n",
      "epoch: 274 loss: 0.009931763634085655\n",
      "epoch: 275 loss: 0.00985531136393547\n",
      "epoch: 276 loss: 0.009780003689229488\n",
      "epoch: 277 loss: 0.00970575399696827\n",
      "epoch: 278 loss: 0.009632088243961334\n",
      "epoch: 279 loss: 0.009559625759720802\n",
      "epoch: 280 loss: 0.009488517418503761\n",
      "epoch: 281 loss: 0.00941769964993\n",
      "epoch: 282 loss: 0.009348086081445217\n",
      "epoch: 283 loss: 0.009279384277760983\n",
      "epoch: 284 loss: 0.009211828000843525\n",
      "epoch: 285 loss: 0.009144828654825687\n",
      "epoch: 286 loss: 0.009078297764062881\n",
      "epoch: 287 loss: 0.009012499824166298\n",
      "epoch: 288 loss: 0.008947789669036865\n",
      "epoch: 289 loss: 0.008883224800229073\n",
      "epoch: 290 loss: 0.008820043876767159\n",
      "epoch: 291 loss: 0.008756916970014572\n",
      "epoch: 292 loss: 0.008694730699062347\n",
      "epoch: 293 loss: 0.008633280172944069\n",
      "epoch: 294 loss: 0.008572121150791645\n",
      "epoch: 295 loss: 0.008511579595506191\n",
      "epoch: 296 loss: 0.00845150463283062\n",
      "epoch: 297 loss: 0.008392373099923134\n",
      "epoch: 298 loss: 0.008333591744303703\n",
      "epoch: 299 loss: 0.008275517262518406\n",
      "epoch: 300 loss: 0.00821785256266594\n",
      "epoch: 301 loss: 0.008160272613167763\n",
      "epoch: 302 loss: 0.008103606291115284\n",
      "epoch: 303 loss: 0.008046995848417282\n",
      "epoch: 304 loss: 0.007991533726453781\n",
      "epoch: 305 loss: 0.007936068810522556\n",
      "epoch: 306 loss: 0.007881222292780876\n",
      "epoch: 307 loss: 0.007826312445104122\n",
      "epoch: 308 loss: 0.007772257551550865\n",
      "epoch: 309 loss: 0.007718376815319061\n",
      "epoch: 310 loss: 0.007665054406970739\n",
      "epoch: 311 loss: 0.007612054701894522\n",
      "epoch: 312 loss: 0.00755949504673481\n",
      "epoch: 313 loss: 0.007507436443120241\n",
      "epoch: 314 loss: 0.00745546305552125\n",
      "epoch: 315 loss: 0.00740398932248354\n",
      "epoch: 316 loss: 0.00735269021242857\n",
      "epoch: 317 loss: 0.007301773875951767\n",
      "epoch: 318 loss: 0.007251298055052757\n",
      "epoch: 319 loss: 0.0072013819590210915\n",
      "epoch: 320 loss: 0.0071519953198730946\n",
      "epoch: 321 loss: 0.007102902512997389\n",
      "epoch: 322 loss: 0.007054014131426811\n",
      "epoch: 323 loss: 0.007005774416029453\n",
      "epoch: 324 loss: 0.006957502570003271\n",
      "epoch: 325 loss: 0.0069098202511668205\n",
      "epoch: 326 loss: 0.00686210673302412\n",
      "epoch: 327 loss: 0.006815130822360516\n",
      "epoch: 328 loss: 0.006768389604985714\n",
      "epoch: 329 loss: 0.006721882615238428\n",
      "epoch: 330 loss: 0.006676054559648037\n",
      "epoch: 331 loss: 0.006630844436585903\n",
      "epoch: 332 loss: 0.006586108356714249\n",
      "epoch: 333 loss: 0.006541517563164234\n",
      "epoch: 334 loss: 0.006496925372630358\n",
      "epoch: 335 loss: 0.006452776025980711\n",
      "epoch: 336 loss: 0.006409483030438423\n",
      "epoch: 337 loss: 0.006366720423102379\n",
      "epoch: 338 loss: 0.0063241044990718365\n",
      "epoch: 339 loss: 0.006281842477619648\n",
      "epoch: 340 loss: 0.006239904090762138\n",
      "epoch: 341 loss: 0.006198614835739136\n",
      "epoch: 342 loss: 0.006158095318824053\n",
      "epoch: 343 loss: 0.0061172787100076675\n",
      "epoch: 344 loss: 0.006076608784496784\n",
      "epoch: 345 loss: 0.006037003360688686\n",
      "epoch: 346 loss: 0.005998019129037857\n",
      "epoch: 347 loss: 0.005959299858659506\n",
      "epoch: 348 loss: 0.005920933559536934\n",
      "epoch: 349 loss: 0.005882773548364639\n",
      "epoch: 350 loss: 0.005845026578754187\n",
      "epoch: 351 loss: 0.005807872861623764\n",
      "epoch: 352 loss: 0.005770716816186905\n",
      "epoch: 353 loss: 0.005734478589147329\n",
      "epoch: 354 loss: 0.005698415916413069\n",
      "epoch: 355 loss: 0.005662767682224512\n",
      "epoch: 356 loss: 0.005627859383821487\n",
      "epoch: 357 loss: 0.005593481473624706\n",
      "epoch: 358 loss: 0.005560020916163921\n",
      "epoch: 359 loss: 0.0055260853841900826\n",
      "epoch: 360 loss: 0.005492830183357\n",
      "epoch: 361 loss: 0.005460373125970364\n",
      "epoch: 362 loss: 0.005428360775113106\n",
      "epoch: 363 loss: 0.005396436434239149\n",
      "epoch: 364 loss: 0.005364926066249609\n",
      "epoch: 365 loss: 0.0053335633128881454\n",
      "epoch: 366 loss: 0.005303000565618277\n",
      "epoch: 367 loss: 0.005272319074720144\n",
      "epoch: 368 loss: 0.005241638049483299\n",
      "epoch: 369 loss: 0.005212051793932915\n",
      "epoch: 370 loss: 0.005182849243283272\n",
      "epoch: 371 loss: 0.005153912119567394\n",
      "epoch: 372 loss: 0.0051255677826702595\n",
      "epoch: 373 loss: 0.005097311921417713\n",
      "epoch: 374 loss: 0.005069292150437832\n",
      "epoch: 375 loss: 0.005041421391069889\n",
      "epoch: 376 loss: 0.005014498718082905\n",
      "epoch: 377 loss: 0.004987841472029686\n",
      "epoch: 378 loss: 0.0049611832946538925\n",
      "epoch: 379 loss: 0.004935265518724918\n",
      "epoch: 380 loss: 0.004909377079457045\n",
      "epoch: 381 loss: 0.004883755464106798\n",
      "epoch: 382 loss: 0.0048587266355752945\n",
      "epoch: 383 loss: 0.004834053106606007\n",
      "epoch: 384 loss: 0.0048097046092152596\n",
      "epoch: 385 loss: 0.004785651806741953\n",
      "epoch: 386 loss: 0.00476201344281435\n",
      "epoch: 387 loss: 0.004738375544548035\n",
      "epoch: 388 loss: 0.0047147078439593315\n",
      "epoch: 389 loss: 0.004691841080784798\n",
      "epoch: 390 loss: 0.0046696849167346954\n",
      "epoch: 391 loss: 0.004647320602089167\n",
      "epoch: 392 loss: 0.004624807275831699\n",
      "epoch: 393 loss: 0.004603093024343252\n",
      "epoch: 394 loss: 0.0045821210369467735\n",
      "epoch: 395 loss: 0.0045608822256326675\n",
      "epoch: 396 loss: 0.004539643879979849\n",
      "epoch: 397 loss: 0.004519056994467974\n",
      "epoch: 398 loss: 0.004498766269534826\n",
      "epoch: 399 loss: 0.004478474147617817\n",
      "epoch: 400 loss: 0.004458597861230373\n",
      "epoch: 401 loss: 0.004438899457454681\n",
      "epoch: 402 loss: 0.004419735167175531\n",
      "epoch: 403 loss: 0.004400569945573807\n",
      "epoch: 404 loss: 0.004381611943244934\n",
      "epoch: 405 loss: 0.004362802021205425\n",
      "epoch: 406 loss: 0.004344732500612736\n",
      "epoch: 407 loss: 0.0043266043066978455\n",
      "epoch: 408 loss: 0.004308654461055994\n",
      "epoch: 409 loss: 0.004291060380637646\n",
      "epoch: 410 loss: 0.004273258615285158\n",
      "epoch: 411 loss: 0.004255900625139475\n",
      "epoch: 412 loss: 0.004238927736878395\n",
      "epoch: 413 loss: 0.0042224000208079815\n",
      "epoch: 414 loss: 0.004205545876175165\n",
      "epoch: 415 loss: 0.004189048428088427\n",
      "epoch: 416 loss: 0.004172906279563904\n",
      "epoch: 417 loss: 0.004156703595072031\n",
      "epoch: 418 loss: 0.004141064826399088\n",
      "epoch: 419 loss: 0.0041252486407756805\n",
      "epoch: 420 loss: 0.004109787754714489\n",
      "epoch: 421 loss: 0.004094267264008522\n",
      "epoch: 422 loss: 0.0040793102234601974\n",
      "epoch: 423 loss: 0.004064501728862524\n",
      "epoch: 424 loss: 0.004049752373248339\n",
      "epoch: 425 loss: 0.00403524050489068\n",
      "epoch: 426 loss: 0.004020994994789362\n",
      "epoch: 427 loss: 0.004006749019026756\n",
      "epoch: 428 loss: 0.0039929477497935295\n",
      "epoch: 429 loss: 0.003978998400270939\n",
      "epoch: 430 loss: 0.003964872099459171\n",
      "epoch: 431 loss: 0.003951219841837883\n",
      "epoch: 432 loss: 0.003937774803489447\n",
      "epoch: 433 loss: 0.003925071097910404\n",
      "epoch: 434 loss: 0.003912011627107859\n",
      "epoch: 435 loss: 0.0038990702014416456\n",
      "epoch: 436 loss: 0.003886188380420208\n",
      "epoch: 437 loss: 0.003873425768688321\n",
      "epoch: 438 loss: 0.0038610780611634254\n",
      "epoch: 439 loss: 0.003848759923130274\n",
      "epoch: 440 loss: 0.003836530726402998\n",
      "epoch: 441 loss: 0.0038242419250309467\n",
      "epoch: 442 loss: 0.0038126646541059017\n",
      "epoch: 443 loss: 0.0038009388372302055\n",
      "epoch: 444 loss: 0.0037893024273216724\n",
      "epoch: 445 loss: 0.003777547972276807\n",
      "epoch: 446 loss: 0.0037660906091332436\n",
      "epoch: 447 loss: 0.0037548993714153767\n",
      "epoch: 448 loss: 0.0037438562139868736\n",
      "epoch: 449 loss: 0.0037328717298805714\n",
      "epoch: 450 loss: 0.003721858374774456\n",
      "epoch: 451 loss: 0.0037111416459083557\n",
      "epoch: 452 loss: 0.0037003064062446356\n",
      "epoch: 453 loss: 0.003689737990498543\n",
      "epoch: 454 loss: 0.0036791397724300623\n",
      "epoch: 455 loss: 0.0036692232824862003\n",
      "epoch: 456 loss: 0.0036590993404388428\n",
      "epoch: 457 loss: 0.003648946061730385\n",
      "epoch: 458 loss: 0.0036388225853443146\n",
      "epoch: 459 loss: 0.003628610400483012\n",
      "epoch: 460 loss: 0.0036188128869980574\n",
      "epoch: 461 loss: 0.003609312232583761\n",
      "epoch: 462 loss: 0.003599751740694046\n",
      "epoch: 463 loss: 0.0035901328083127737\n",
      "epoch: 464 loss: 0.0035807802341878414\n",
      "epoch: 465 loss: 0.003571220440790057\n",
      "epoch: 466 loss: 0.0035618976689875126\n",
      "epoch: 467 loss: 0.0035530487075448036\n",
      "epoch: 468 loss: 0.003543933853507042\n",
      "epoch: 469 loss: 0.003534848801791668\n",
      "epoch: 470 loss: 0.0035261791199445724\n",
      "epoch: 471 loss: 0.0035174498334527016\n",
      "epoch: 472 loss: 0.003508749883621931\n",
      "epoch: 473 loss: 0.003500168677419424\n",
      "epoch: 474 loss: 0.0034917949233204126\n",
      "epoch: 475 loss: 0.003483332460746169\n",
      "epoch: 476 loss: 0.0034749892074614763\n",
      "epoch: 477 loss: 0.003466853639110923\n",
      "epoch: 478 loss: 0.0034587769769132137\n",
      "epoch: 479 loss: 0.0034504923969507217\n",
      "epoch: 480 loss: 0.0034427118953317404\n",
      "epoch: 481 loss: 0.0034349611960351467\n",
      "epoch: 482 loss: 0.003427152056246996\n",
      "epoch: 483 loss: 0.0034189866855740547\n",
      "epoch: 484 loss: 0.003411384765058756\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 485 loss: 0.00340354535728693\n",
      "epoch: 486 loss: 0.003395646344870329\n",
      "epoch: 487 loss: 0.0033880737610161304\n",
      "epoch: 488 loss: 0.00338088721036911\n",
      "epoch: 489 loss: 0.0033734035678207874\n",
      "epoch: 490 loss: 0.0033660694025456905\n",
      "epoch: 491 loss: 0.003358853282406926\n",
      "epoch: 492 loss: 0.0033517256379127502\n",
      "epoch: 493 loss: 0.003344687633216381\n",
      "epoch: 494 loss: 0.003337530419230461\n",
      "epoch: 495 loss: 0.0033305520191788673\n",
      "epoch: 496 loss: 0.0033236625604331493\n",
      "epoch: 497 loss: 0.00331689091399312\n",
      "epoch: 498 loss: 0.003310149535536766\n",
      "epoch: 499 loss: 0.0033031413331627846\n",
      "tensor([[-0.1705,  6.7791, -0.1152,  ..., -3.0895, -3.2151, -3.1585],\n",
      "        [ 2.0374,  3.1059, -0.2052,  ..., -3.1716, -3.1443, -3.1539],\n",
      "        [ 1.7285,  3.1283,  0.9923,  ..., -3.1005, -3.2036, -3.1178],\n",
      "        ...,\n",
      "        [ 2.1793,  0.7096,  1.7521,  ..., -2.9551, -2.9261, -2.9966],\n",
      "        [ 1.9546,  2.6964,  1.0602,  ..., -2.8685, -2.9935, -2.8596],\n",
      "        [ 1.4250,  2.8942,  1.2392,  ..., -2.7956, -2.9375, -2.8055]],\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SAGE(torch.nn.Module):\n",
    "    def __init__(self, in_c, hid_c, out_c):\n",
    "        super(SAGE,self).__init__()    # 构造函数\n",
    "\n",
    "        self.conv1 = SAGEConv(in_channels=in_c, out_channels=hid_c)\n",
    "        self.conv2 = SAGEConv(in_channels=hid_c, out_channels=out_c)\n",
    "  \n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index    # x:节点特征矩阵; edge_index:COO格式的图形连接，维度[2,边的数量]，数据：[ [源节点],[目标节点] ]\n",
    "        \n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.conv2(x, edge_index)\n",
    "        \n",
    "        x1 = F.log_softmax(x, dim=1)\n",
    "        \n",
    "        return x1, x\n",
    "\n",
    "model = SAGE(in_c=dataset.num_node_features,hid_c=100,out_c=dataset.num_node_features)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n",
    "print(model(data))\n",
    "\n",
    "model.train()\n",
    "for epoch in range(500):\n",
    "    optimizer.zero_grad()\n",
    "    out1, out = model(data)\n",
    "    # 交叉熵损失\n",
    "    loss = F.nll_loss(out1[data.train_mask], data.y[data.train_mask])       # 负对数似然。\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print('epoch:',epoch, 'loss:',loss.item())  \n",
    "\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f54fecf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calinski-Harabaz Index： 14.700669094454609\n",
      "Silhouette Coefficient： 0.35009575\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn import cluster\n",
    "from sklearn import metrics\n",
    "\n",
    "out=out.cpu().detach().numpy()\n",
    "model = cluster.KMeans(n_clusters=4)\n",
    "model.fit(out)\n",
    "\n",
    "y_predict = model.predict(out)\n",
    "\n",
    "# Calinski-Harabaz Index：越大越好\n",
    "# Silhouette Coefficient：轮廓系数（越大越好)\n",
    "print('Calinski-Harabaz Index：',metrics.calinski_harabasz_score(out,y_predict))\n",
    "print('Silhouette Coefficient：',metrics.silhouette_score(out,y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8822180",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>170.709259</td>\n",
       "      <td>16.812178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9.037280</td>\n",
       "      <td>39.837204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>61.188454</td>\n",
       "      <td>-28.854555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-28.987829</td>\n",
       "      <td>-23.672665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-133.402161</td>\n",
       "      <td>-147.067001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>52.495529</td>\n",
       "      <td>150.127151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>60.521305</td>\n",
       "      <td>-82.852417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-32.260651</td>\n",
       "      <td>24.366505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-181.942215</td>\n",
       "      <td>36.280582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-28.685383</td>\n",
       "      <td>79.784592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-42.295025</td>\n",
       "      <td>-136.073135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-154.248611</td>\n",
       "      <td>-65.753838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-92.754860</td>\n",
       "      <td>-89.729271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>54.408360</td>\n",
       "      <td>31.601959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-122.517654</td>\n",
       "      <td>98.026306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-69.989365</td>\n",
       "      <td>122.051056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>89.003311</td>\n",
       "      <td>-131.550537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-79.638969</td>\n",
       "      <td>-42.296059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-120.068047</td>\n",
       "      <td>40.877720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-124.924820</td>\n",
       "      <td>-15.776328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>104.068398</td>\n",
       "      <td>48.498493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24.464334</td>\n",
       "      <td>87.850754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-74.432716</td>\n",
       "      <td>63.811588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>26.546824</td>\n",
       "      <td>-152.617386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>178.574417</td>\n",
       "      <td>-79.591438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>114.316353</td>\n",
       "      <td>-67.102303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-13.517019</td>\n",
       "      <td>137.792755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13.178645</td>\n",
       "      <td>-51.983521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.815904</td>\n",
       "      <td>-100.553757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>81.297546</td>\n",
       "      <td>95.083191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-73.484367</td>\n",
       "      <td>7.848786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-35.384056</td>\n",
       "      <td>-75.548988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19.324305</td>\n",
       "      <td>-6.319637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>107.085480</td>\n",
       "      <td>-10.091620</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0           1\n",
       "3  170.709259   16.812178\n",
       "3    9.037280   39.837204\n",
       "3   61.188454  -28.854555\n",
       "3  -28.987829  -23.672665\n",
       "1 -133.402161 -147.067001\n",
       "1   52.495529  150.127151\n",
       "3   60.521305  -82.852417\n",
       "1  -32.260651   24.366505\n",
       "0 -181.942215   36.280582\n",
       "0  -28.685383   79.784592\n",
       "3  -42.295025 -136.073135\n",
       "1 -154.248611  -65.753838\n",
       "1  -92.754860  -89.729271\n",
       "1   54.408360   31.601959\n",
       "0 -122.517654   98.026306\n",
       "0  -69.989365  122.051056\n",
       "3   89.003311 -131.550537\n",
       "1  -79.638969  -42.296059\n",
       "0 -120.068047   40.877720\n",
       "1 -124.924820  -15.776328\n",
       "0  104.068398   48.498493\n",
       "1   24.464334   87.850754\n",
       "0  -74.432716   63.811588\n",
       "0   26.546824 -152.617386\n",
       "2  178.574417  -79.591438\n",
       "0  114.316353  -67.102303\n",
       "0  -13.517019  137.792755\n",
       "0   13.178645  -51.983521\n",
       "0    8.815904 -100.553757\n",
       "0   81.297546   95.083191\n",
       "3  -73.484367    7.848786\n",
       "0  -35.384056  -75.548988\n",
       "3   19.324305   -6.319637\n",
       "3  107.085480  -10.091620"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "t_sne = TSNE()\n",
    "data=t_sne.fit_transform(out)\n",
    "data=pd.DataFrame(data)\n",
    "data = pd.DataFrame(data,index=y_predict)\n",
    "data_tsne = pd.DataFrame(t_sne.embedding_, index =y_predict)\n",
    "data_tsne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bbf2d941",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7fe26cd0da90>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "d = data_tsne[data_tsne.index == 0]     #找出聚类类别为0的数据对应的降维结果\n",
    "plt.scatter(d[0], d[1],c='lightgreen',marker='o')\n",
    "d = data_tsne[data_tsne.index == 1]\n",
    "plt.scatter(d[0], d[1], c='orange',\tmarker='o')\n",
    "d = data_tsne[data_tsne.index == 2]\n",
    "plt.scatter(d[0], d[1], c='lightblue',marker='o')\n",
    "d = data_tsne[data_tsne.index == 3]\n",
    "plt.scatter(d[0], d[1], c='red',marker='o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2c6478",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
