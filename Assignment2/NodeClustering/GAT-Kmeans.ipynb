{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9fc6f813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset: KarateClub():\n",
      "======================\n",
      "Number of graphs: 1\n",
      "Number of features: 34\n",
      "Number of classes: 4\n",
      "\n",
      "Data(x=[34, 34], edge_index=[2, 156], y=[34], train_mask=[34])\n",
      "======================\n",
      "Number of nodes: 34\n",
      "Number of edges: 156\n",
      "Average node degree: 4.59\n",
      "Number of training nodes: 4\n",
      "Training node label rate: 0.12\n",
      "Contains isolated nodes: False\n",
      "Contains self-loops: False\n",
      "Is undirected: True\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.datasets import NELL\n",
    "from torch_geometric.datasets import KarateClub\n",
    "from torch_geometric.transforms import NormalizeFeatures\n",
    "from torch_geometric.nn import GCNConv, SAGEConv, GATConv\n",
    "import torch\n",
    "\n",
    "dataset = KarateClub()\n",
    "\n",
    "print()\n",
    "print(f'Dataset: {dataset}:')\n",
    "print('======================')\n",
    "print(f'Number of graphs: {len(dataset)}')\n",
    "print(f'Number of features: {dataset.num_features}')\n",
    "print(f'Number of classes: {dataset.num_classes}')\n",
    "data = dataset[0]\n",
    "# data = dataset[0]  # Get the first graph object.\n",
    "\n",
    "print()\n",
    "print(data)\n",
    "print('======================')\n",
    "\n",
    "# Gather some statistics about the graph.\n",
    "print(f'Number of nodes: {data.num_nodes}')\n",
    "print(f'Number of edges: {data.num_edges}')\n",
    "print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
    "print(f'Number of training nodes: {data.train_mask.sum()}')\n",
    "print(f'Training node label rate: {int(data.train_mask.sum()) / data.num_nodes:.2f}')\n",
    "print(f'Contains isolated nodes: {data.has_isolated_nodes()}')\n",
    "print(f'Contains self-loops: {data.has_self_loops()}')\n",
    "print(f'Is undirected: {data.is_undirected()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e89aa00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[-3.5951, -3.5336, -3.6310,  ..., -3.4709, -3.5619, -3.5976],\n",
      "        [-3.5482, -3.5157, -3.5829,  ..., -3.5039, -3.5670, -3.5508],\n",
      "        [-3.5545, -3.5155, -3.6052,  ..., -3.5279, -3.5720, -3.5684],\n",
      "        ...,\n",
      "        [-3.5640, -3.5059, -3.5695,  ..., -3.5108, -3.5491, -3.5659],\n",
      "        [-3.5708, -3.5797, -3.6752,  ..., -3.5233, -3.5095, -3.5829],\n",
      "        [-3.5853, -3.5799, -3.6937,  ..., -3.5201, -3.5231, -3.6080]],\n",
      "       grad_fn=<LogSoftmaxBackward>), tensor([[-0.0831, -0.0216, -0.1190,  ...,  0.0410, -0.0499, -0.0856],\n",
      "        [-0.0303,  0.0022, -0.0650,  ...,  0.0140, -0.0491, -0.0329],\n",
      "        [-0.0352,  0.0038, -0.0858,  ..., -0.0086, -0.0526, -0.0491],\n",
      "        ...,\n",
      "        [-0.0439,  0.0143, -0.0493,  ...,  0.0094, -0.0289, -0.0457],\n",
      "        [-0.0472, -0.0561, -0.1516,  ...,  0.0004,  0.0141, -0.0593],\n",
      "        [-0.0606, -0.0553, -0.1691,  ...,  0.0046,  0.0015, -0.0834]],\n",
      "       grad_fn=<AddBackward0>))\n",
      "epoch: 0 loss: 3.520704507827759\n",
      "epoch: 1 loss: 3.506716012954712\n",
      "epoch: 2 loss: 3.492799997329712\n",
      "epoch: 3 loss: 3.478935956954956\n",
      "epoch: 4 loss: 3.465031385421753\n",
      "epoch: 5 loss: 3.4511284828186035\n",
      "epoch: 6 loss: 3.4372737407684326\n",
      "epoch: 7 loss: 3.4233574867248535\n",
      "epoch: 8 loss: 3.409435272216797\n",
      "epoch: 9 loss: 3.3954708576202393\n",
      "epoch: 10 loss: 3.381430149078369\n",
      "epoch: 11 loss: 3.367257833480835\n",
      "epoch: 12 loss: 3.3529787063598633\n",
      "epoch: 13 loss: 3.338578939437866\n",
      "epoch: 14 loss: 3.3240556716918945\n",
      "epoch: 15 loss: 3.3092994689941406\n",
      "epoch: 16 loss: 3.2942519187927246\n",
      "epoch: 17 loss: 3.2789812088012695\n",
      "epoch: 18 loss: 3.263418674468994\n",
      "epoch: 19 loss: 3.247497797012329\n",
      "epoch: 20 loss: 3.2312541007995605\n",
      "epoch: 21 loss: 3.2145631313323975\n",
      "epoch: 22 loss: 3.1974034309387207\n",
      "epoch: 23 loss: 3.1797966957092285\n",
      "epoch: 24 loss: 3.1617133617401123\n",
      "epoch: 25 loss: 3.143218994140625\n",
      "epoch: 26 loss: 3.1242423057556152\n",
      "epoch: 27 loss: 3.104745626449585\n",
      "epoch: 28 loss: 3.084693431854248\n",
      "epoch: 29 loss: 3.0640416145324707\n",
      "epoch: 30 loss: 3.0428242683410645\n",
      "epoch: 31 loss: 3.0211305618286133\n",
      "epoch: 32 loss: 2.9989242553710938\n",
      "epoch: 33 loss: 2.9762094020843506\n",
      "epoch: 34 loss: 2.95294451713562\n",
      "epoch: 35 loss: 2.929107189178467\n",
      "epoch: 36 loss: 2.904698133468628\n",
      "epoch: 37 loss: 2.879725217819214\n",
      "epoch: 38 loss: 2.8541531562805176\n",
      "epoch: 39 loss: 2.827878713607788\n",
      "epoch: 40 loss: 2.8010425567626953\n",
      "epoch: 41 loss: 2.7736685276031494\n",
      "epoch: 42 loss: 2.7457494735717773\n",
      "epoch: 43 loss: 2.7173495292663574\n",
      "epoch: 44 loss: 2.688488483428955\n",
      "epoch: 45 loss: 2.659151792526245\n",
      "epoch: 46 loss: 2.6293091773986816\n",
      "epoch: 47 loss: 2.598996162414551\n",
      "epoch: 48 loss: 2.5682644844055176\n",
      "epoch: 49 loss: 2.537166118621826\n",
      "epoch: 50 loss: 2.505697011947632\n",
      "epoch: 51 loss: 2.473893165588379\n",
      "epoch: 52 loss: 2.441761016845703\n",
      "epoch: 53 loss: 2.409365653991699\n",
      "epoch: 54 loss: 2.3767216205596924\n",
      "epoch: 55 loss: 2.343862533569336\n",
      "epoch: 56 loss: 2.310812473297119\n",
      "epoch: 57 loss: 2.2776529788970947\n",
      "epoch: 58 loss: 2.2444252967834473\n",
      "epoch: 59 loss: 2.2111339569091797\n",
      "epoch: 60 loss: 2.177847146987915\n",
      "epoch: 61 loss: 2.1446189880371094\n",
      "epoch: 62 loss: 2.111506223678589\n",
      "epoch: 63 loss: 2.0785374641418457\n",
      "epoch: 64 loss: 2.0457470417022705\n",
      "epoch: 65 loss: 2.0131444931030273\n",
      "epoch: 66 loss: 1.9807648658752441\n",
      "epoch: 67 loss: 1.9486802816390991\n",
      "epoch: 68 loss: 1.9169586896896362\n",
      "epoch: 69 loss: 1.8856189250946045\n",
      "epoch: 70 loss: 1.8546795845031738\n",
      "epoch: 71 loss: 1.8241567611694336\n",
      "epoch: 72 loss: 1.7940726280212402\n",
      "epoch: 73 loss: 1.7644338607788086\n",
      "epoch: 74 loss: 1.7352441549301147\n",
      "epoch: 75 loss: 1.7065497636795044\n",
      "epoch: 76 loss: 1.6783459186553955\n",
      "epoch: 77 loss: 1.6506330966949463\n",
      "epoch: 78 loss: 1.623432993888855\n",
      "epoch: 79 loss: 1.5967378616333008\n",
      "epoch: 80 loss: 1.5705549716949463\n",
      "epoch: 81 loss: 1.5448685884475708\n",
      "epoch: 82 loss: 1.519670844078064\n",
      "epoch: 83 loss: 1.4949713945388794\n",
      "epoch: 84 loss: 1.4707551002502441\n",
      "epoch: 85 loss: 1.4470311403274536\n",
      "epoch: 86 loss: 1.4238289594650269\n",
      "epoch: 87 loss: 1.4011354446411133\n",
      "epoch: 88 loss: 1.378946304321289\n",
      "epoch: 89 loss: 1.357245922088623\n",
      "epoch: 90 loss: 1.3360074758529663\n",
      "epoch: 91 loss: 1.3152273893356323\n",
      "epoch: 92 loss: 1.294895052909851\n",
      "epoch: 93 loss: 1.2749927043914795\n",
      "epoch: 94 loss: 1.2554851770401\n",
      "epoch: 95 loss: 1.2363853454589844\n",
      "epoch: 96 loss: 1.2176532745361328\n",
      "epoch: 97 loss: 1.1992961168289185\n",
      "epoch: 98 loss: 1.1813271045684814\n",
      "epoch: 99 loss: 1.1637166738510132\n",
      "epoch: 100 loss: 1.146469235420227\n",
      "epoch: 101 loss: 1.1295485496520996\n",
      "epoch: 102 loss: 1.1129329204559326\n",
      "epoch: 103 loss: 1.0966262817382812\n",
      "epoch: 104 loss: 1.0806167125701904\n",
      "epoch: 105 loss: 1.064895749092102\n",
      "epoch: 106 loss: 1.0494580268859863\n",
      "epoch: 107 loss: 1.0342923402786255\n",
      "epoch: 108 loss: 1.0193809270858765\n",
      "epoch: 109 loss: 1.0047101974487305\n",
      "epoch: 110 loss: 0.9902723431587219\n",
      "epoch: 111 loss: 0.9760690331459045\n",
      "epoch: 112 loss: 0.9620766639709473\n",
      "epoch: 113 loss: 0.9482979774475098\n",
      "epoch: 114 loss: 0.9347323179244995\n",
      "epoch: 115 loss: 0.9213711023330688\n",
      "epoch: 116 loss: 0.908198356628418\n",
      "epoch: 117 loss: 0.8952131271362305\n",
      "epoch: 118 loss: 0.8824144005775452\n",
      "epoch: 119 loss: 0.869804322719574\n",
      "epoch: 120 loss: 0.8573684096336365\n",
      "epoch: 121 loss: 0.8451070189476013\n",
      "epoch: 122 loss: 0.8330233097076416\n",
      "epoch: 123 loss: 0.8211060762405396\n",
      "epoch: 124 loss: 0.8093451857566833\n",
      "epoch: 125 loss: 0.7977268099784851\n",
      "epoch: 126 loss: 0.786262571811676\n",
      "epoch: 127 loss: 0.7749532461166382\n",
      "epoch: 128 loss: 0.7637982368469238\n",
      "epoch: 129 loss: 0.7527890801429749\n",
      "epoch: 130 loss: 0.7419344186782837\n",
      "epoch: 131 loss: 0.7312201261520386\n",
      "epoch: 132 loss: 0.7206439971923828\n",
      "epoch: 133 loss: 0.7102112174034119\n",
      "epoch: 134 loss: 0.6999102234840393\n",
      "epoch: 135 loss: 0.6897435188293457\n",
      "epoch: 136 loss: 0.6797124147415161\n",
      "epoch: 137 loss: 0.6698122024536133\n",
      "epoch: 138 loss: 0.66005939245224\n",
      "epoch: 139 loss: 0.6504389047622681\n",
      "epoch: 140 loss: 0.6409545540809631\n",
      "epoch: 141 loss: 0.6316098570823669\n",
      "epoch: 142 loss: 0.622390866279602\n",
      "epoch: 143 loss: 0.6133090853691101\n",
      "epoch: 144 loss: 0.6043609380722046\n",
      "epoch: 145 loss: 0.595529317855835\n",
      "epoch: 146 loss: 0.5868301391601562\n",
      "epoch: 147 loss: 0.5782541036605835\n",
      "epoch: 148 loss: 0.5697921514511108\n",
      "epoch: 149 loss: 0.5614498257637024\n",
      "epoch: 150 loss: 0.5532292127609253\n",
      "epoch: 151 loss: 0.5451326370239258\n",
      "epoch: 152 loss: 0.5371620059013367\n",
      "epoch: 153 loss: 0.5293075442314148\n",
      "epoch: 154 loss: 0.5215704441070557\n",
      "epoch: 155 loss: 0.5139467716217041\n",
      "epoch: 156 loss: 0.5064384937286377\n",
      "epoch: 157 loss: 0.49904686212539673\n",
      "epoch: 158 loss: 0.49176639318466187\n",
      "epoch: 159 loss: 0.48459354043006897\n",
      "epoch: 160 loss: 0.47753292322158813\n",
      "epoch: 161 loss: 0.4705744683742523\n",
      "epoch: 162 loss: 0.463717520236969\n",
      "epoch: 163 loss: 0.45696383714675903\n",
      "epoch: 164 loss: 0.4503113627433777\n",
      "epoch: 165 loss: 0.4437658190727234\n",
      "epoch: 166 loss: 0.4373266100883484\n",
      "epoch: 167 loss: 0.4309830963611603\n",
      "epoch: 168 loss: 0.42474299669265747\n",
      "epoch: 169 loss: 0.4185977578163147\n",
      "epoch: 170 loss: 0.41255423426628113\n",
      "epoch: 171 loss: 0.4066045582294464\n",
      "epoch: 172 loss: 0.4007493853569031\n",
      "epoch: 173 loss: 0.394992470741272\n",
      "epoch: 174 loss: 0.38932371139526367\n",
      "epoch: 175 loss: 0.38374561071395874\n",
      "epoch: 176 loss: 0.37825578451156616\n",
      "epoch: 177 loss: 0.3728540539741516\n",
      "epoch: 178 loss: 0.36754634976387024\n",
      "epoch: 179 loss: 0.3623266816139221\n",
      "epoch: 180 loss: 0.3571925163269043\n",
      "epoch: 181 loss: 0.3521440029144287\n",
      "epoch: 182 loss: 0.3471790552139282\n",
      "epoch: 183 loss: 0.34229418635368347\n",
      "epoch: 184 loss: 0.33749011158943176\n",
      "epoch: 185 loss: 0.33276498317718506\n",
      "epoch: 186 loss: 0.32811716198921204\n",
      "epoch: 187 loss: 0.3235470652580261\n",
      "epoch: 188 loss: 0.31904932856559753\n",
      "epoch: 189 loss: 0.3146263360977173\n",
      "epoch: 190 loss: 0.3102799952030182\n",
      "epoch: 191 loss: 0.30600568652153015\n",
      "epoch: 192 loss: 0.301801472902298\n",
      "epoch: 193 loss: 0.2976682782173157\n",
      "epoch: 194 loss: 0.29360195994377136\n",
      "epoch: 195 loss: 0.28960227966308594\n",
      "epoch: 196 loss: 0.28566864132881165\n",
      "epoch: 197 loss: 0.2818009555339813\n",
      "epoch: 198 loss: 0.2779974043369293\n",
      "epoch: 199 loss: 0.2742585837841034\n",
      "epoch: 200 loss: 0.27058470249176025\n",
      "epoch: 201 loss: 0.2669687271118164\n",
      "epoch: 202 loss: 0.26341453194618225\n",
      "epoch: 203 loss: 0.2599170207977295\n",
      "epoch: 204 loss: 0.2564763128757477\n",
      "epoch: 205 loss: 0.2530944049358368\n",
      "epoch: 206 loss: 0.249769926071167\n",
      "epoch: 207 loss: 0.24650365114212036\n",
      "epoch: 208 loss: 0.24329477548599243\n",
      "epoch: 209 loss: 0.24013981223106384\n",
      "epoch: 210 loss: 0.2370358407497406\n",
      "epoch: 211 loss: 0.23398059606552124\n",
      "epoch: 212 loss: 0.23097562789916992\n",
      "epoch: 213 loss: 0.22802168130874634\n",
      "epoch: 214 loss: 0.22511576116085052\n",
      "epoch: 215 loss: 0.2222580909729004\n",
      "epoch: 216 loss: 0.21945087611675262\n",
      "epoch: 217 loss: 0.21668870747089386\n",
      "epoch: 218 loss: 0.21397140622138977\n",
      "epoch: 219 loss: 0.2112979143857956\n",
      "epoch: 220 loss: 0.20866908133029938\n",
      "epoch: 221 loss: 0.2060823142528534\n",
      "epoch: 222 loss: 0.2035384327173233\n",
      "epoch: 223 loss: 0.20103630423545837\n",
      "epoch: 224 loss: 0.19857436418533325\n",
      "epoch: 225 loss: 0.1961521953344345\n",
      "epoch: 226 loss: 0.19376978278160095\n",
      "epoch: 227 loss: 0.1914265900850296\n",
      "epoch: 228 loss: 0.18912085890769958\n",
      "epoch: 229 loss: 0.1868516057729721\n",
      "epoch: 230 loss: 0.18461987376213074\n",
      "epoch: 231 loss: 0.18242445588111877\n",
      "epoch: 232 loss: 0.18026210367679596\n",
      "epoch: 233 loss: 0.17813445627689362\n",
      "epoch: 234 loss: 0.17603977024555206\n",
      "epoch: 235 loss: 0.1739789843559265\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 236 loss: 0.17195086181163788\n",
      "epoch: 237 loss: 0.16995379328727722\n",
      "epoch: 238 loss: 0.16799308359622955\n",
      "epoch: 239 loss: 0.1660623848438263\n",
      "epoch: 240 loss: 0.1641615629196167\n",
      "epoch: 241 loss: 0.16229046881198883\n",
      "epoch: 242 loss: 0.16044874489307404\n",
      "epoch: 243 loss: 0.15863636136054993\n",
      "epoch: 244 loss: 0.15685142576694489\n",
      "epoch: 245 loss: 0.15509429574012756\n",
      "epoch: 246 loss: 0.1533636450767517\n",
      "epoch: 247 loss: 0.15166190266609192\n",
      "epoch: 248 loss: 0.14998692274093628\n",
      "epoch: 249 loss: 0.14833879470825195\n",
      "epoch: 250 loss: 0.1467139720916748\n",
      "epoch: 251 loss: 0.1451144963502884\n",
      "epoch: 252 loss: 0.1435399353504181\n",
      "epoch: 253 loss: 0.14198920130729675\n",
      "epoch: 254 loss: 0.14046058058738708\n",
      "epoch: 255 loss: 0.13895533978939056\n",
      "epoch: 256 loss: 0.13747245073318481\n",
      "epoch: 257 loss: 0.13601166009902954\n",
      "epoch: 258 loss: 0.13457360863685608\n",
      "epoch: 259 loss: 0.13315577805042267\n",
      "epoch: 260 loss: 0.13176053762435913\n",
      "epoch: 261 loss: 0.1303853839635849\n",
      "epoch: 262 loss: 0.12903156876564026\n",
      "epoch: 263 loss: 0.12769633531570435\n",
      "epoch: 264 loss: 0.12637996673583984\n",
      "epoch: 265 loss: 0.1250835508108139\n",
      "epoch: 266 loss: 0.12380567938089371\n",
      "epoch: 267 loss: 0.12254518270492554\n",
      "epoch: 268 loss: 0.1213042289018631\n",
      "epoch: 269 loss: 0.1200806275010109\n",
      "epoch: 270 loss: 0.11887502670288086\n",
      "epoch: 271 loss: 0.11768686771392822\n",
      "epoch: 272 loss: 0.11651462316513062\n",
      "epoch: 273 loss: 0.11535961925983429\n",
      "epoch: 274 loss: 0.11422078311443329\n",
      "epoch: 275 loss: 0.11309954524040222\n",
      "epoch: 276 loss: 0.11199220269918442\n",
      "epoch: 277 loss: 0.11090121418237686\n",
      "epoch: 278 loss: 0.10982605814933777\n",
      "epoch: 279 loss: 0.10876594483852386\n",
      "epoch: 280 loss: 0.10772082209587097\n",
      "epoch: 281 loss: 0.10669023543596268\n",
      "epoch: 282 loss: 0.10567353665828705\n",
      "epoch: 283 loss: 0.10467123985290527\n",
      "epoch: 284 loss: 0.10368206351995468\n",
      "epoch: 285 loss: 0.10270651429891586\n",
      "epoch: 286 loss: 0.10174451768398285\n",
      "epoch: 287 loss: 0.1007973775267601\n",
      "epoch: 288 loss: 0.09986218810081482\n",
      "epoch: 289 loss: 0.09894009679555893\n",
      "epoch: 290 loss: 0.09803083539009094\n",
      "epoch: 291 loss: 0.097134068608284\n",
      "epoch: 292 loss: 0.09624946862459183\n",
      "epoch: 293 loss: 0.09537816047668457\n",
      "epoch: 294 loss: 0.09451637417078018\n",
      "epoch: 295 loss: 0.09366670250892639\n",
      "epoch: 296 loss: 0.0928293988108635\n",
      "epoch: 297 loss: 0.09200295060873032\n",
      "epoch: 298 loss: 0.09118738025426865\n",
      "epoch: 299 loss: 0.09038359671831131\n",
      "epoch: 300 loss: 0.08959019184112549\n",
      "epoch: 301 loss: 0.08880828320980072\n",
      "epoch: 302 loss: 0.08803592622280121\n",
      "epoch: 303 loss: 0.08727462589740753\n",
      "epoch: 304 loss: 0.08652284741401672\n",
      "epoch: 305 loss: 0.08578085899353027\n",
      "epoch: 306 loss: 0.08504746854305267\n",
      "epoch: 307 loss: 0.08432522416114807\n",
      "epoch: 308 loss: 0.08361169695854187\n",
      "epoch: 309 loss: 0.08290839195251465\n",
      "epoch: 310 loss: 0.08221554756164551\n",
      "epoch: 311 loss: 0.08153083175420761\n",
      "epoch: 312 loss: 0.08085368573665619\n",
      "epoch: 313 loss: 0.0801856741309166\n",
      "epoch: 314 loss: 0.07952567934989929\n",
      "epoch: 315 loss: 0.0788736343383789\n",
      "epoch: 316 loss: 0.07823045551776886\n",
      "epoch: 317 loss: 0.077596016228199\n",
      "epoch: 318 loss: 0.076969675719738\n",
      "epoch: 319 loss: 0.07635053247213364\n",
      "epoch: 320 loss: 0.07573902606964111\n",
      "epoch: 321 loss: 0.07513760775327682\n",
      "epoch: 322 loss: 0.07454350590705872\n",
      "epoch: 323 loss: 0.07395646721124649\n",
      "epoch: 324 loss: 0.07337579131126404\n",
      "epoch: 325 loss: 0.07280175387859344\n",
      "epoch: 326 loss: 0.07223605364561081\n",
      "epoch: 327 loss: 0.0716768205165863\n",
      "epoch: 328 loss: 0.07112377136945724\n",
      "epoch: 329 loss: 0.07057830691337585\n",
      "epoch: 330 loss: 0.0700397789478302\n",
      "epoch: 331 loss: 0.0695071816444397\n",
      "epoch: 332 loss: 0.068980373442173\n",
      "epoch: 333 loss: 0.06846030801534653\n",
      "epoch: 334 loss: 0.06794633716344833\n",
      "epoch: 335 loss: 0.06743929535150528\n",
      "epoch: 336 loss: 0.06693849712610245\n",
      "epoch: 337 loss: 0.0664437785744667\n",
      "epoch: 338 loss: 0.06595446914434433\n",
      "epoch: 339 loss: 0.0654701218008995\n",
      "epoch: 340 loss: 0.06499162316322327\n",
      "epoch: 341 loss: 0.06451880931854248\n",
      "epoch: 342 loss: 0.06405161321163177\n",
      "epoch: 343 loss: 0.06359008699655533\n",
      "epoch: 344 loss: 0.0631348192691803\n",
      "epoch: 345 loss: 0.06268478184938431\n",
      "epoch: 346 loss: 0.06223943084478378\n",
      "epoch: 347 loss: 0.0617995411157608\n",
      "epoch: 348 loss: 0.0613640695810318\n",
      "epoch: 349 loss: 0.06093450263142586\n",
      "epoch: 350 loss: 0.06050962582230568\n",
      "epoch: 351 loss: 0.06008918955922127\n",
      "epoch: 352 loss: 0.0596732534468174\n",
      "epoch: 353 loss: 0.059262219816446304\n",
      "epoch: 354 loss: 0.0588558092713356\n",
      "epoch: 355 loss: 0.058453597128391266\n",
      "epoch: 356 loss: 0.05805608257651329\n",
      "epoch: 357 loss: 0.057662658393383026\n",
      "epoch: 358 loss: 0.05727361515164375\n",
      "epoch: 359 loss: 0.05688939616084099\n",
      "epoch: 360 loss: 0.05650921165943146\n",
      "epoch: 361 loss: 0.056133147329092026\n",
      "epoch: 362 loss: 0.05576107278466225\n",
      "epoch: 363 loss: 0.055393584072589874\n",
      "epoch: 364 loss: 0.05502946674823761\n",
      "epoch: 365 loss: 0.0546690970659256\n",
      "epoch: 366 loss: 0.05431303009390831\n",
      "epoch: 367 loss: 0.05396115034818649\n",
      "epoch: 368 loss: 0.0536118783056736\n",
      "epoch: 369 loss: 0.053266994655132294\n",
      "epoch: 370 loss: 0.05292621627449989\n",
      "epoch: 371 loss: 0.05258840695023537\n",
      "epoch: 372 loss: 0.05225427821278572\n",
      "epoch: 373 loss: 0.05192270502448082\n",
      "epoch: 374 loss: 0.05159466713666916\n",
      "epoch: 375 loss: 0.05127006769180298\n",
      "epoch: 376 loss: 0.05094902962446213\n",
      "epoch: 377 loss: 0.050630491226911545\n",
      "epoch: 378 loss: 0.05031575635075569\n",
      "epoch: 379 loss: 0.050004296004772186\n",
      "epoch: 380 loss: 0.04969589412212372\n",
      "epoch: 381 loss: 0.04939074069261551\n",
      "epoch: 382 loss: 0.04908864200115204\n",
      "epoch: 383 loss: 0.048789869993925095\n",
      "epoch: 384 loss: 0.04849327728152275\n",
      "epoch: 385 loss: 0.04819973558187485\n",
      "epoch: 386 loss: 0.04790911078453064\n",
      "epoch: 387 loss: 0.0476217046380043\n",
      "epoch: 388 loss: 0.04733720421791077\n",
      "epoch: 389 loss: 0.04705509543418884\n",
      "epoch: 390 loss: 0.046775996685028076\n",
      "epoch: 391 loss: 0.04649878293275833\n",
      "epoch: 392 loss: 0.046224430203437805\n",
      "epoch: 393 loss: 0.045952294021844864\n",
      "epoch: 394 loss: 0.045682430267333984\n",
      "epoch: 395 loss: 0.045414891093969345\n",
      "epoch: 396 loss: 0.04514989256858826\n",
      "epoch: 397 loss: 0.04488725587725639\n",
      "epoch: 398 loss: 0.044627416878938675\n",
      "epoch: 399 loss: 0.044370122253894806\n",
      "epoch: 400 loss: 0.04411475360393524\n",
      "epoch: 401 loss: 0.04386179521679878\n",
      "epoch: 402 loss: 0.043610990047454834\n",
      "epoch: 403 loss: 0.04336212947964668\n",
      "epoch: 404 loss: 0.043115243315696716\n",
      "epoch: 405 loss: 0.04287131503224373\n",
      "epoch: 406 loss: 0.04262933507561684\n",
      "epoch: 407 loss: 0.04238978028297424\n",
      "epoch: 408 loss: 0.04215221107006073\n",
      "epoch: 409 loss: 0.04191725701093674\n",
      "epoch: 410 loss: 0.04168447107076645\n",
      "epoch: 411 loss: 0.041454024612903595\n",
      "epoch: 412 loss: 0.041225288063287735\n",
      "epoch: 413 loss: 0.04099865257740021\n",
      "epoch: 414 loss: 0.04077351093292236\n",
      "epoch: 415 loss: 0.040550246834754944\n",
      "epoch: 416 loss: 0.04032944515347481\n",
      "epoch: 417 loss: 0.040109895169734955\n",
      "epoch: 418 loss: 0.03989283740520477\n",
      "epoch: 419 loss: 0.03967783600091934\n",
      "epoch: 420 loss: 0.0394648052752018\n",
      "epoch: 421 loss: 0.03925326466560364\n",
      "epoch: 422 loss: 0.03904348611831665\n",
      "epoch: 423 loss: 0.03883594274520874\n",
      "epoch: 424 loss: 0.03863045573234558\n",
      "epoch: 425 loss: 0.03842616081237793\n",
      "epoch: 426 loss: 0.038223594427108765\n",
      "epoch: 427 loss: 0.03802323341369629\n",
      "epoch: 428 loss: 0.03782430291175842\n",
      "epoch: 429 loss: 0.037627145648002625\n",
      "epoch: 430 loss: 0.037431616336107254\n",
      "epoch: 431 loss: 0.03723752498626709\n",
      "epoch: 432 loss: 0.0370454415678978\n",
      "epoch: 433 loss: 0.036855489015579224\n",
      "epoch: 434 loss: 0.03666672855615616\n",
      "epoch: 435 loss: 0.036479175090789795\n",
      "epoch: 436 loss: 0.036293432116508484\n",
      "epoch: 437 loss: 0.03610893711447716\n",
      "epoch: 438 loss: 0.03592675179243088\n",
      "epoch: 439 loss: 0.035745829343795776\n",
      "epoch: 440 loss: 0.035566382110118866\n",
      "epoch: 441 loss: 0.03538832813501358\n",
      "epoch: 442 loss: 0.03521162271499634\n",
      "epoch: 443 loss: 0.03503682091832161\n",
      "epoch: 444 loss: 0.03486340120434761\n",
      "epoch: 445 loss: 0.03469109907746315\n",
      "epoch: 446 loss: 0.03452088683843613\n",
      "epoch: 447 loss: 0.03435184061527252\n",
      "epoch: 448 loss: 0.03418378904461861\n",
      "epoch: 449 loss: 0.034017257392406464\n",
      "epoch: 450 loss: 0.03385239839553833\n",
      "epoch: 451 loss: 0.03368891030550003\n",
      "epoch: 452 loss: 0.03352709114551544\n",
      "epoch: 453 loss: 0.03336646780371666\n",
      "epoch: 454 loss: 0.03320734202861786\n",
      "epoch: 455 loss: 0.03304870054125786\n",
      "epoch: 456 loss: 0.03289096802473068\n",
      "epoch: 457 loss: 0.03273516148328781\n",
      "epoch: 458 loss: 0.03258084878325462\n",
      "epoch: 459 loss: 0.03242780268192291\n",
      "epoch: 460 loss: 0.03227616101503372\n",
      "epoch: 461 loss: 0.03212590888142586\n",
      "epoch: 462 loss: 0.03197680413722992\n",
      "epoch: 463 loss: 0.0318288616836071\n",
      "epoch: 464 loss: 0.03168173134326935\n",
      "epoch: 465 loss: 0.03153607249259949\n",
      "epoch: 466 loss: 0.0313916876912117\n",
      "epoch: 467 loss: 0.031248901039361954\n",
      "epoch: 468 loss: 0.03110704943537712\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 469 loss: 0.03096626326441765\n",
      "epoch: 470 loss: 0.030826617032289505\n",
      "epoch: 471 loss: 0.030688278377056122\n",
      "epoch: 472 loss: 0.03055058792233467\n",
      "epoch: 473 loss: 0.030414260923862457\n",
      "epoch: 474 loss: 0.030279263854026794\n",
      "epoch: 475 loss: 0.030144743621349335\n",
      "epoch: 476 loss: 0.030011728405952454\n",
      "epoch: 477 loss: 0.02987983077764511\n",
      "epoch: 478 loss: 0.029749643057584763\n",
      "epoch: 479 loss: 0.029619820415973663\n",
      "epoch: 480 loss: 0.029490584507584572\n",
      "epoch: 481 loss: 0.02936314046382904\n",
      "epoch: 482 loss: 0.029236964881420135\n",
      "epoch: 483 loss: 0.029111545532941818\n",
      "epoch: 484 loss: 0.02898668311536312\n",
      "epoch: 485 loss: 0.02886296808719635\n",
      "epoch: 486 loss: 0.028740154579281807\n",
      "epoch: 487 loss: 0.02861853316426277\n",
      "epoch: 488 loss: 0.028498098254203796\n",
      "epoch: 489 loss: 0.02837861329317093\n",
      "epoch: 490 loss: 0.02825971692800522\n",
      "epoch: 491 loss: 0.028141994029283524\n",
      "epoch: 492 loss: 0.028025083243846893\n",
      "epoch: 493 loss: 0.027909521013498306\n",
      "epoch: 494 loss: 0.027794592082500458\n",
      "epoch: 495 loss: 0.02768063172698021\n",
      "epoch: 496 loss: 0.027567438781261444\n",
      "epoch: 497 loss: 0.027455125004053116\n",
      "epoch: 498 loss: 0.027343904599547386\n",
      "epoch: 499 loss: 0.027233419939875603\n",
      "tensor([[ 2.3663,  7.0411,  1.1508,  ..., -5.7807, -5.9117, -5.9995],\n",
      "        [ 3.8092,  5.4904,  0.7069,  ..., -4.1388, -4.2344, -4.2603],\n",
      "        [ 5.4486,  4.0981,  1.8887,  ..., -4.1658, -4.2740, -4.2474],\n",
      "        ...,\n",
      "        [ 3.4381,  1.9923,  4.6025,  ..., -3.4084, -3.5513, -3.4696],\n",
      "        [ 8.8998,  2.7276,  2.3242,  ..., -4.8656, -4.9274, -4.9520],\n",
      "        [ 9.4736,  3.8361,  3.2995,  ..., -5.9148, -6.0490, -6.0518]],\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self, in_c, hid_c, out_c):\n",
    "        super(GAT,self).__init__()    # 构造函数\n",
    "\n",
    "        self.conv1 = GCNConv(in_channels=in_c, out_channels=hid_c)\n",
    "        self.conv2 = GCNConv(in_channels=hid_c, out_channels=out_c)\n",
    "  \n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index    # x:节点特征矩阵; edge_index:COO格式的图形连接，维度[2,边的数量]，数据：[ [源节点],[目标节点] ]\n",
    "        \n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.conv2(x, edge_index)\n",
    "        \n",
    "        x1 = F.log_softmax(x, dim=1)\n",
    "        \n",
    "        return x1, x\n",
    "\n",
    "model = GAT(in_c=dataset.num_node_features,hid_c=100,out_c=dataset.num_node_features)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n",
    "\n",
    "print(model(data))\n",
    "\n",
    "model.train()\n",
    "for epoch in range(500):\n",
    "    optimizer.zero_grad()\n",
    "    out1, out = model(data)\n",
    "    # 交叉熵损失\n",
    "    loss = F.nll_loss(out1[data.train_mask], data.y[data.train_mask])       # 负对数似然。\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print('epoch:',epoch, 'loss:',loss.item())  \n",
    "\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eccf2f30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calinski-Harabaz Index： 38.46212906145406\n",
      "Silhouette Coefficient： 0.4816\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn import cluster\n",
    "from sklearn import metrics\n",
    "\n",
    "out=out.cpu().detach().numpy()\n",
    "model = cluster.KMeans(n_clusters=4)\n",
    "model.fit(out)\n",
    "\n",
    "y_predict = model.predict(out)\n",
    "\n",
    "# Calinski-Harabaz Index：越大越好\n",
    "# Silhouette Coefficient：轮廓系数（越大越好)\n",
    "print('Calinski-Harabaz Index：',metrics.calinski_harabasz_score(out,y_predict))\n",
    "print('Silhouette Coefficient：',metrics.silhouette_score(out,y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9af7df60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12.631412</td>\n",
       "      <td>74.320702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-24.364256</td>\n",
       "      <td>52.330948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-3.214460</td>\n",
       "      <td>52.606445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-22.385569</td>\n",
       "      <td>16.921469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-61.899113</td>\n",
       "      <td>-1.362327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-66.611839</td>\n",
       "      <td>23.195040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-54.033928</td>\n",
       "      <td>41.592388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-19.960463</td>\n",
       "      <td>-0.818222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>32.524612</td>\n",
       "      <td>39.424358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>29.584946</td>\n",
       "      <td>2.134078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-37.804176</td>\n",
       "      <td>30.046305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-4.842752</td>\n",
       "      <td>5.691754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-27.741804</td>\n",
       "      <td>-40.098110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23.987974</td>\n",
       "      <td>22.336920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>47.316471</td>\n",
       "      <td>-8.314093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.143444</td>\n",
       "      <td>-47.932693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-48.462433</td>\n",
       "      <td>14.760221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-6.264313</td>\n",
       "      <td>-12.457401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26.820963</td>\n",
       "      <td>-16.242105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19.559496</td>\n",
       "      <td>-39.896854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>45.151733</td>\n",
       "      <td>15.160166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-24.643869</td>\n",
       "      <td>-19.070196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>39.401897</td>\n",
       "      <td>-30.718464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.266266</td>\n",
       "      <td>37.998253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-37.517750</td>\n",
       "      <td>-1.562232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-14.678922</td>\n",
       "      <td>32.959259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12.747585</td>\n",
       "      <td>9.568478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-9.210508</td>\n",
       "      <td>-30.334251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.652333</td>\n",
       "      <td>-6.704103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.596591</td>\n",
       "      <td>22.767099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.281933</td>\n",
       "      <td>-24.267134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-45.362507</td>\n",
       "      <td>-20.600746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-9.695177</td>\n",
       "      <td>72.311523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-29.720854</td>\n",
       "      <td>78.680573</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0          1\n",
       "2  12.631412  74.320702\n",
       "2 -24.364256  52.330948\n",
       "2  -3.214460  52.606445\n",
       "3 -22.385569  16.921469\n",
       "0 -61.899113  -1.362327\n",
       "0 -66.611839  23.195040\n",
       "0 -54.033928  41.592388\n",
       "1 -19.960463  -0.818222\n",
       "1  32.524612  39.424358\n",
       "1  29.584946   2.134078\n",
       "0 -37.804176  30.046305\n",
       "1  -4.842752   5.691754\n",
       "1 -27.741804 -40.098110\n",
       "1  23.987974  22.336920\n",
       "1  47.316471  -8.314093\n",
       "1  -0.143444 -47.932693\n",
       "0 -48.462433  14.760221\n",
       "1  -6.264313 -12.457401\n",
       "1  26.820963 -16.242105\n",
       "1  19.559496 -39.896854\n",
       "1  45.151733  15.160166\n",
       "1 -24.643869 -19.070196\n",
       "1  39.401897 -30.718464\n",
       "3  10.266266  37.998253\n",
       "3 -37.517750  -1.562232\n",
       "3 -14.678922  32.959259\n",
       "1  12.747585   9.568478\n",
       "3  -9.210508 -30.334251\n",
       "1  10.652333  -6.704103\n",
       "1   0.596591  22.767099\n",
       "1   9.281933 -24.267134\n",
       "3 -45.362507 -20.600746\n",
       "2  -9.695177  72.311523\n",
       "2 -29.720854  78.680573"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "t_sne = TSNE()\n",
    "data=t_sne.fit_transform(out)\n",
    "data=pd.DataFrame(data)\n",
    "data = pd.DataFrame(data,index=y_predict)\n",
    "data_tsne = pd.DataFrame(t_sne.embedding_, index =y_predict)\n",
    "data_tsne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd7bd47c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f861d42da90>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "d = data_tsne[data_tsne.index == 0]     #找出聚类类别为0的数据对应的降维结果\n",
    "plt.scatter(d[0], d[1],c='lightgreen',marker='o')\n",
    "d = data_tsne[data_tsne.index == 1]\n",
    "plt.scatter(d[0], d[1], c='orange',\tmarker='o')\n",
    "d = data_tsne[data_tsne.index == 2]\n",
    "plt.scatter(d[0], d[1], c='lightblue',marker='o')\n",
    "d = data_tsne[data_tsne.index == 3]\n",
    "plt.scatter(d[0], d[1], c='red',marker='o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbed1c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
