{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f3abfbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset: KarateClub():\n",
      "======================\n",
      "Number of graphs: 1\n",
      "Number of features: 34\n",
      "Number of classes: 4\n",
      "\n",
      "Data(x=[34, 34], edge_index=[2, 156], y=[34], train_mask=[34])\n",
      "======================\n",
      "Number of nodes: 34\n",
      "Number of edges: 156\n",
      "Average node degree: 4.59\n",
      "Number of training nodes: 4\n",
      "Training node label rate: 0.12\n",
      "Contains isolated nodes: False\n",
      "Contains self-loops: False\n",
      "Is undirected: True\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.datasets import NELL\n",
    "from torch_geometric.datasets import KarateClub\n",
    "from torch_geometric.transforms import NormalizeFeatures\n",
    "from torch_geometric.nn import GCNConv, SAGEConv, GATConv\n",
    "import torch\n",
    "\n",
    "dataset = KarateClub()\n",
    "\n",
    "print()\n",
    "print(f'Dataset: {dataset}:')\n",
    "print('======================')\n",
    "print(f'Number of graphs: {len(dataset)}')\n",
    "print(f'Number of features: {dataset.num_features}')\n",
    "print(f'Number of classes: {dataset.num_classes}')\n",
    "data = dataset[0]\n",
    "# data = dataset[0]  # Get the first graph object.\n",
    "\n",
    "print()\n",
    "print(data)\n",
    "print('======================')\n",
    "\n",
    "# Gather some statistics about the graph.\n",
    "print(f'Number of nodes: {data.num_nodes}')\n",
    "print(f'Number of edges: {data.num_edges}')\n",
    "print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
    "print(f'Number of training nodes: {data.train_mask.sum()}')\n",
    "print(f'Training node label rate: {int(data.train_mask.sum()) / data.num_nodes:.2f}')\n",
    "print(f'Contains isolated nodes: {data.has_isolated_nodes()}')\n",
    "print(f'Contains self-loops: {data.has_self_loops()}')\n",
    "print(f'Is undirected: {data.is_undirected()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c829680d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[-3.5694, -3.5350, -3.4424,  ..., -3.4999, -3.5030, -3.5552],\n",
      "        [-3.5675, -3.5345, -3.4692,  ..., -3.4943, -3.4970, -3.5409],\n",
      "        [-3.5549, -3.5613, -3.4735,  ..., -3.5040, -3.5179, -3.5502],\n",
      "        ...,\n",
      "        [-3.5349, -3.5582, -3.4964,  ..., -3.5137, -3.5456, -3.5494],\n",
      "        [-3.5608, -3.5750, -3.4676,  ..., -3.5397, -3.5249, -3.5546],\n",
      "        [-3.5770, -3.5796, -3.4526,  ..., -3.5344, -3.5357, -3.5546]],\n",
      "       grad_fn=<LogSoftmaxBackward>), tensor([[-0.0406, -0.0062,  0.0864,  ...,  0.0289,  0.0258, -0.0264],\n",
      "        [-0.0424, -0.0093,  0.0559,  ...,  0.0309,  0.0281, -0.0158],\n",
      "        [-0.0293, -0.0357,  0.0520,  ...,  0.0216,  0.0077, -0.0247],\n",
      "        ...,\n",
      "        [-0.0082, -0.0316,  0.0303,  ...,  0.0129, -0.0190, -0.0228],\n",
      "        [-0.0308, -0.0449,  0.0625,  ..., -0.0096,  0.0051, -0.0246],\n",
      "        [-0.0477, -0.0503,  0.0766,  ..., -0.0051, -0.0064, -0.0253]],\n",
      "       grad_fn=<AddBackward0>))\n",
      "epoch: 0 loss: 3.517864227294922\n",
      "epoch: 1 loss: 3.504940986633301\n",
      "epoch: 2 loss: 3.492034673690796\n",
      "epoch: 3 loss: 3.4791336059570312\n",
      "epoch: 4 loss: 3.4662070274353027\n",
      "epoch: 5 loss: 3.4531967639923096\n",
      "epoch: 6 loss: 3.440062999725342\n",
      "epoch: 7 loss: 3.4268081188201904\n",
      "epoch: 8 loss: 3.413431406021118\n",
      "epoch: 9 loss: 3.3999366760253906\n",
      "epoch: 10 loss: 3.38626766204834\n",
      "epoch: 11 loss: 3.372375965118408\n",
      "epoch: 12 loss: 3.3582286834716797\n",
      "epoch: 13 loss: 3.3437604904174805\n",
      "epoch: 14 loss: 3.329005479812622\n",
      "epoch: 15 loss: 3.313990592956543\n",
      "epoch: 16 loss: 3.29856014251709\n",
      "epoch: 17 loss: 3.2827529907226562\n",
      "epoch: 18 loss: 3.266491413116455\n",
      "epoch: 19 loss: 3.2498302459716797\n",
      "epoch: 20 loss: 3.2327687740325928\n",
      "epoch: 21 loss: 3.2152016162872314\n",
      "epoch: 22 loss: 3.1970913410186768\n",
      "epoch: 23 loss: 3.178480625152588\n",
      "epoch: 24 loss: 3.159325122833252\n",
      "epoch: 25 loss: 3.139569044113159\n",
      "epoch: 26 loss: 3.1192028522491455\n",
      "epoch: 27 loss: 3.0982275009155273\n",
      "epoch: 28 loss: 3.076658248901367\n",
      "epoch: 29 loss: 3.054413080215454\n",
      "epoch: 30 loss: 3.031541109085083\n",
      "epoch: 31 loss: 3.0080671310424805\n",
      "epoch: 32 loss: 2.9839584827423096\n",
      "epoch: 33 loss: 2.959256410598755\n",
      "epoch: 34 loss: 2.9339706897735596\n",
      "epoch: 35 loss: 2.908071756362915\n",
      "epoch: 36 loss: 2.8815248012542725\n",
      "epoch: 37 loss: 2.8543319702148438\n",
      "epoch: 38 loss: 2.826441526412964\n",
      "epoch: 39 loss: 2.7978219985961914\n",
      "epoch: 40 loss: 2.7685556411743164\n",
      "epoch: 41 loss: 2.7386298179626465\n",
      "epoch: 42 loss: 2.708111047744751\n",
      "epoch: 43 loss: 2.6769959926605225\n",
      "epoch: 44 loss: 2.6452715396881104\n",
      "epoch: 45 loss: 2.6130175590515137\n",
      "epoch: 46 loss: 2.580195903778076\n",
      "epoch: 47 loss: 2.5468897819519043\n",
      "epoch: 48 loss: 2.51314115524292\n",
      "epoch: 49 loss: 2.4789950847625732\n",
      "epoch: 50 loss: 2.444495677947998\n",
      "epoch: 51 loss: 2.4096341133117676\n",
      "epoch: 52 loss: 2.3744735717773438\n",
      "epoch: 53 loss: 2.3390655517578125\n",
      "epoch: 54 loss: 2.303480625152588\n",
      "epoch: 55 loss: 2.2677464485168457\n",
      "epoch: 56 loss: 2.2319204807281494\n",
      "epoch: 57 loss: 2.196087598800659\n",
      "epoch: 58 loss: 2.1603307723999023\n",
      "epoch: 59 loss: 2.1247031688690186\n",
      "epoch: 60 loss: 2.089254379272461\n",
      "epoch: 61 loss: 2.0540430545806885\n",
      "epoch: 62 loss: 2.019105911254883\n",
      "epoch: 63 loss: 1.984483242034912\n",
      "epoch: 64 loss: 1.9502222537994385\n",
      "epoch: 65 loss: 1.9163358211517334\n",
      "epoch: 66 loss: 1.8828864097595215\n",
      "epoch: 67 loss: 1.8499101400375366\n",
      "epoch: 68 loss: 1.817428469657898\n",
      "epoch: 69 loss: 1.7854443788528442\n",
      "epoch: 70 loss: 1.7540178298950195\n",
      "epoch: 71 loss: 1.723154067993164\n",
      "epoch: 72 loss: 1.692838191986084\n",
      "epoch: 73 loss: 1.6630771160125732\n",
      "epoch: 74 loss: 1.6338708400726318\n",
      "epoch: 75 loss: 1.6052544116973877\n",
      "epoch: 76 loss: 1.5772238969802856\n",
      "epoch: 77 loss: 1.5497846603393555\n",
      "epoch: 78 loss: 1.522942066192627\n",
      "epoch: 79 loss: 1.4966918230056763\n",
      "epoch: 80 loss: 1.4710267782211304\n",
      "epoch: 81 loss: 1.4459471702575684\n",
      "epoch: 82 loss: 1.421468734741211\n",
      "epoch: 83 loss: 1.3975645303726196\n",
      "epoch: 84 loss: 1.374207854270935\n",
      "epoch: 85 loss: 1.35140061378479\n",
      "epoch: 86 loss: 1.329126000404358\n",
      "epoch: 87 loss: 1.307386040687561\n",
      "epoch: 88 loss: 1.2861710786819458\n",
      "epoch: 89 loss: 1.2654625177383423\n",
      "epoch: 90 loss: 1.2452329397201538\n",
      "epoch: 91 loss: 1.2254806756973267\n",
      "epoch: 92 loss: 1.2061961889266968\n",
      "epoch: 93 loss: 1.1873575448989868\n",
      "epoch: 94 loss: 1.1689517498016357\n",
      "epoch: 95 loss: 1.1509596109390259\n",
      "epoch: 96 loss: 1.1333763599395752\n",
      "epoch: 97 loss: 1.1161953210830688\n",
      "epoch: 98 loss: 1.0993937253952026\n",
      "epoch: 99 loss: 1.0829542875289917\n",
      "epoch: 100 loss: 1.0668638944625854\n",
      "epoch: 101 loss: 1.0511056184768677\n",
      "epoch: 102 loss: 1.0356736183166504\n",
      "epoch: 103 loss: 1.0205488204956055\n",
      "epoch: 104 loss: 1.0057185888290405\n",
      "epoch: 105 loss: 0.9911580085754395\n",
      "epoch: 106 loss: 0.9768680334091187\n",
      "epoch: 107 loss: 0.962843120098114\n",
      "epoch: 108 loss: 0.9490613341331482\n",
      "epoch: 109 loss: 0.935511589050293\n",
      "epoch: 110 loss: 0.9221842288970947\n",
      "epoch: 111 loss: 0.9090734720230103\n",
      "epoch: 112 loss: 0.8961806893348694\n",
      "epoch: 113 loss: 0.8835024237632751\n",
      "epoch: 114 loss: 0.8710297346115112\n",
      "epoch: 115 loss: 0.8587440848350525\n",
      "epoch: 116 loss: 0.8466425538063049\n",
      "epoch: 117 loss: 0.8347246050834656\n",
      "epoch: 118 loss: 0.8229801058769226\n",
      "epoch: 119 loss: 0.8114341497421265\n",
      "epoch: 120 loss: 0.8000791072845459\n",
      "epoch: 121 loss: 0.7888911366462708\n",
      "epoch: 122 loss: 0.7778645753860474\n",
      "epoch: 123 loss: 0.7670021057128906\n",
      "epoch: 124 loss: 0.7562917470932007\n",
      "epoch: 125 loss: 0.7457379102706909\n",
      "epoch: 126 loss: 0.7353456616401672\n",
      "epoch: 127 loss: 0.7251074314117432\n",
      "epoch: 128 loss: 0.7150232791900635\n",
      "epoch: 129 loss: 0.7050789594650269\n",
      "epoch: 130 loss: 0.695271909236908\n",
      "epoch: 131 loss: 0.6856020092964172\n",
      "epoch: 132 loss: 0.6760631799697876\n",
      "epoch: 133 loss: 0.6666556596755981\n",
      "epoch: 134 loss: 0.6573757529258728\n",
      "epoch: 135 loss: 0.648217499256134\n",
      "epoch: 136 loss: 0.6391794085502625\n",
      "epoch: 137 loss: 0.6302672624588013\n",
      "epoch: 138 loss: 0.6214832067489624\n",
      "epoch: 139 loss: 0.6128156781196594\n",
      "epoch: 140 loss: 0.6042649745941162\n",
      "epoch: 141 loss: 0.5958268642425537\n",
      "epoch: 142 loss: 0.5875061750411987\n",
      "epoch: 143 loss: 0.579296350479126\n",
      "epoch: 144 loss: 0.571202278137207\n",
      "epoch: 145 loss: 0.5632203221321106\n",
      "epoch: 146 loss: 0.5553537011146545\n",
      "epoch: 147 loss: 0.5475833415985107\n",
      "epoch: 148 loss: 0.5399174094200134\n",
      "epoch: 149 loss: 0.5323660373687744\n",
      "epoch: 150 loss: 0.524919867515564\n",
      "epoch: 151 loss: 0.5175725221633911\n",
      "epoch: 152 loss: 0.5103310942649841\n",
      "epoch: 153 loss: 0.5031934976577759\n",
      "epoch: 154 loss: 0.49615678191185\n",
      "epoch: 155 loss: 0.4892154932022095\n",
      "epoch: 156 loss: 0.482371985912323\n",
      "epoch: 157 loss: 0.4756208658218384\n",
      "epoch: 158 loss: 0.468966007232666\n",
      "epoch: 159 loss: 0.4624028205871582\n",
      "epoch: 160 loss: 0.4559335708618164\n",
      "epoch: 161 loss: 0.4495561420917511\n",
      "epoch: 162 loss: 0.4432690441608429\n",
      "epoch: 163 loss: 0.4370763301849365\n",
      "epoch: 164 loss: 0.4309829771518707\n",
      "epoch: 165 loss: 0.4249827265739441\n",
      "epoch: 166 loss: 0.4190704822540283\n",
      "epoch: 167 loss: 0.4132384657859802\n",
      "epoch: 168 loss: 0.4074927568435669\n",
      "epoch: 169 loss: 0.40183374285697937\n",
      "epoch: 170 loss: 0.39625683426856995\n",
      "epoch: 171 loss: 0.3907608091831207\n",
      "epoch: 172 loss: 0.38534852862358093\n",
      "epoch: 173 loss: 0.38001900911331177\n",
      "epoch: 174 loss: 0.3747704327106476\n",
      "epoch: 175 loss: 0.3695985674858093\n",
      "epoch: 176 loss: 0.3645021319389343\n",
      "epoch: 177 loss: 0.35948410630226135\n",
      "epoch: 178 loss: 0.3545458912849426\n",
      "epoch: 179 loss: 0.3496803045272827\n",
      "epoch: 180 loss: 0.3448910415172577\n",
      "epoch: 181 loss: 0.34017235040664673\n",
      "epoch: 182 loss: 0.33552566170692444\n",
      "epoch: 183 loss: 0.3309507966041565\n",
      "epoch: 184 loss: 0.32644689083099365\n",
      "epoch: 185 loss: 0.3220159411430359\n",
      "epoch: 186 loss: 0.3176548182964325\n",
      "epoch: 187 loss: 0.3133634328842163\n",
      "epoch: 188 loss: 0.3091387450695038\n",
      "epoch: 189 loss: 0.3049811124801636\n",
      "epoch: 190 loss: 0.3008977472782135\n",
      "epoch: 191 loss: 0.2968777120113373\n",
      "epoch: 192 loss: 0.2929264008998871\n",
      "epoch: 193 loss: 0.2890373766422272\n",
      "epoch: 194 loss: 0.28521159291267395\n",
      "epoch: 195 loss: 0.2814464867115021\n",
      "epoch: 196 loss: 0.27774012088775635\n",
      "epoch: 197 loss: 0.27409613132476807\n",
      "epoch: 198 loss: 0.27051106095314026\n",
      "epoch: 199 loss: 0.26698341965675354\n",
      "epoch: 200 loss: 0.26351261138916016\n",
      "epoch: 201 loss: 0.26009827852249146\n",
      "epoch: 202 loss: 0.2567394971847534\n",
      "epoch: 203 loss: 0.2534327507019043\n",
      "epoch: 204 loss: 0.25017526745796204\n",
      "epoch: 205 loss: 0.2469700574874878\n",
      "epoch: 206 loss: 0.2438185065984726\n",
      "epoch: 207 loss: 0.24071791768074036\n",
      "epoch: 208 loss: 0.23766624927520752\n",
      "epoch: 209 loss: 0.2346615493297577\n",
      "epoch: 210 loss: 0.23170709609985352\n",
      "epoch: 211 loss: 0.22879916429519653\n",
      "epoch: 212 loss: 0.2259381115436554\n",
      "epoch: 213 loss: 0.22312326729297638\n",
      "epoch: 214 loss: 0.22035342454910278\n",
      "epoch: 215 loss: 0.217629075050354\n",
      "epoch: 216 loss: 0.21494795382022858\n",
      "epoch: 217 loss: 0.21230830252170563\n",
      "epoch: 218 loss: 0.20970992743968964\n",
      "epoch: 219 loss: 0.20715218782424927\n",
      "epoch: 220 loss: 0.20463894307613373\n",
      "epoch: 221 loss: 0.2021685391664505\n",
      "epoch: 222 loss: 0.19973896443843842\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 223 loss: 0.19734612107276917\n",
      "epoch: 224 loss: 0.19499030709266663\n",
      "epoch: 225 loss: 0.19267234206199646\n",
      "epoch: 226 loss: 0.19038952887058258\n",
      "epoch: 227 loss: 0.188144713640213\n",
      "epoch: 228 loss: 0.18593531847000122\n",
      "epoch: 229 loss: 0.18376097083091736\n",
      "epoch: 230 loss: 0.18162105977535248\n",
      "epoch: 231 loss: 0.1795162558555603\n",
      "epoch: 232 loss: 0.17744645476341248\n",
      "epoch: 233 loss: 0.17540927231311798\n",
      "epoch: 234 loss: 0.173405259847641\n",
      "epoch: 235 loss: 0.17143326997756958\n",
      "epoch: 236 loss: 0.16949467360973358\n",
      "epoch: 237 loss: 0.16758567094802856\n",
      "epoch: 238 loss: 0.16570618748664856\n",
      "epoch: 239 loss: 0.16385763883590698\n",
      "epoch: 240 loss: 0.16203781962394714\n",
      "epoch: 241 loss: 0.1602514684200287\n",
      "epoch: 242 loss: 0.15849155187606812\n",
      "epoch: 243 loss: 0.15676215291023254\n",
      "epoch: 244 loss: 0.1550593376159668\n",
      "epoch: 245 loss: 0.15338343381881714\n",
      "epoch: 246 loss: 0.15173554420471191\n",
      "epoch: 247 loss: 0.15011201798915863\n",
      "epoch: 248 loss: 0.14851294457912445\n",
      "epoch: 249 loss: 0.14693906903266907\n",
      "epoch: 250 loss: 0.14538902044296265\n",
      "epoch: 251 loss: 0.14386308193206787\n",
      "epoch: 252 loss: 0.14235876500606537\n",
      "epoch: 253 loss: 0.1408773511648178\n",
      "epoch: 254 loss: 0.1394200623035431\n",
      "epoch: 255 loss: 0.1379867047071457\n",
      "epoch: 256 loss: 0.1365749090909958\n",
      "epoch: 257 loss: 0.13518495857715607\n",
      "epoch: 258 loss: 0.13381563127040863\n",
      "epoch: 259 loss: 0.1324668824672699\n",
      "epoch: 260 loss: 0.1311381757259369\n",
      "epoch: 261 loss: 0.12982812523841858\n",
      "epoch: 262 loss: 0.12853677570819855\n",
      "epoch: 263 loss: 0.12726609408855438\n",
      "epoch: 264 loss: 0.12601426243782043\n",
      "epoch: 265 loss: 0.12478028982877731\n",
      "epoch: 266 loss: 0.12356721609830856\n",
      "epoch: 267 loss: 0.12237059324979782\n",
      "epoch: 268 loss: 0.12119176983833313\n",
      "epoch: 269 loss: 0.12002973258495331\n",
      "epoch: 270 loss: 0.11888375878334045\n",
      "epoch: 271 loss: 0.11775475740432739\n",
      "epoch: 272 loss: 0.11664216220378876\n",
      "epoch: 273 loss: 0.11554595082998276\n",
      "epoch: 274 loss: 0.11446450650691986\n",
      "epoch: 275 loss: 0.11339782178401947\n",
      "epoch: 276 loss: 0.11234665662050247\n",
      "epoch: 277 loss: 0.11130985617637634\n",
      "epoch: 278 loss: 0.11028856039047241\n",
      "epoch: 279 loss: 0.10928122699260712\n",
      "epoch: 280 loss: 0.10828831791877747\n",
      "epoch: 281 loss: 0.10730933398008347\n",
      "epoch: 282 loss: 0.10634297132492065\n",
      "epoch: 283 loss: 0.10538977384567261\n",
      "epoch: 284 loss: 0.104449063539505\n",
      "epoch: 285 loss: 0.10352085530757904\n",
      "epoch: 286 loss: 0.10260429978370667\n",
      "epoch: 287 loss: 0.10170096904039383\n",
      "epoch: 288 loss: 0.1008082926273346\n",
      "epoch: 289 loss: 0.09992826730012894\n",
      "epoch: 290 loss: 0.09905963391065598\n",
      "epoch: 291 loss: 0.09820188581943512\n",
      "epoch: 292 loss: 0.097355917096138\n",
      "epoch: 293 loss: 0.09652134776115417\n",
      "epoch: 294 loss: 0.09569722414016724\n",
      "epoch: 295 loss: 0.09488410502672195\n",
      "epoch: 296 loss: 0.09408170729875565\n",
      "epoch: 297 loss: 0.09328918159008026\n",
      "epoch: 298 loss: 0.09250659495592117\n",
      "epoch: 299 loss: 0.0917343944311142\n",
      "epoch: 300 loss: 0.09097345173358917\n",
      "epoch: 301 loss: 0.09022174030542374\n",
      "epoch: 302 loss: 0.08947944641113281\n",
      "epoch: 303 loss: 0.08874615281820297\n",
      "epoch: 304 loss: 0.08802321553230286\n",
      "epoch: 305 loss: 0.08731041848659515\n",
      "epoch: 306 loss: 0.08660616725683212\n",
      "epoch: 307 loss: 0.08591019362211227\n",
      "epoch: 308 loss: 0.08522430062294006\n",
      "epoch: 309 loss: 0.08454681932926178\n",
      "epoch: 310 loss: 0.08387818932533264\n",
      "epoch: 311 loss: 0.08321719616651535\n",
      "epoch: 312 loss: 0.08256495743989944\n",
      "epoch: 313 loss: 0.08192084729671478\n",
      "epoch: 314 loss: 0.08128447085618973\n",
      "epoch: 315 loss: 0.08065654337406158\n",
      "epoch: 316 loss: 0.08003613352775574\n",
      "epoch: 317 loss: 0.07942280173301697\n",
      "epoch: 318 loss: 0.07881806045770645\n",
      "epoch: 319 loss: 0.07822001725435257\n",
      "epoch: 320 loss: 0.07762917131185532\n",
      "epoch: 321 loss: 0.07704576104879379\n",
      "epoch: 322 loss: 0.07646918296813965\n",
      "epoch: 323 loss: 0.07589913159608841\n",
      "epoch: 324 loss: 0.07533574849367142\n",
      "epoch: 325 loss: 0.07477962970733643\n",
      "epoch: 326 loss: 0.07423048466444016\n",
      "epoch: 327 loss: 0.07368680089712143\n",
      "epoch: 328 loss: 0.07315002381801605\n",
      "epoch: 329 loss: 0.07262022793292999\n",
      "epoch: 330 loss: 0.07209602743387222\n",
      "epoch: 331 loss: 0.07157788425683975\n",
      "epoch: 332 loss: 0.07106593251228333\n",
      "epoch: 333 loss: 0.07055984437465668\n",
      "epoch: 334 loss: 0.07006002217531204\n",
      "epoch: 335 loss: 0.06956593692302704\n",
      "epoch: 336 loss: 0.06907770037651062\n",
      "epoch: 337 loss: 0.06859411299228668\n",
      "epoch: 338 loss: 0.06811697036027908\n",
      "epoch: 339 loss: 0.06764525175094604\n",
      "epoch: 340 loss: 0.06717889755964279\n",
      "epoch: 341 loss: 0.0667177140712738\n",
      "epoch: 342 loss: 0.06626181304454803\n",
      "epoch: 343 loss: 0.06581072509288788\n",
      "epoch: 344 loss: 0.06536471098661423\n",
      "epoch: 345 loss: 0.0649237111210823\n",
      "epoch: 346 loss: 0.06448731571435928\n",
      "epoch: 347 loss: 0.064055897295475\n",
      "epoch: 348 loss: 0.06362885981798172\n",
      "epoch: 349 loss: 0.0632062554359436\n",
      "epoch: 350 loss: 0.06278830766677856\n",
      "epoch: 351 loss: 0.06237535923719406\n",
      "epoch: 352 loss: 0.061966732144355774\n",
      "epoch: 353 loss: 0.06156183034181595\n",
      "epoch: 354 loss: 0.06116193160414696\n",
      "epoch: 355 loss: 0.06076602265238762\n",
      "epoch: 356 loss: 0.06037452816963196\n",
      "epoch: 357 loss: 0.05998704209923744\n",
      "epoch: 358 loss: 0.05960358679294586\n",
      "epoch: 359 loss: 0.05922475457191467\n",
      "epoch: 360 loss: 0.05884948745369911\n",
      "epoch: 361 loss: 0.058478131890296936\n",
      "epoch: 362 loss: 0.0581110343337059\n",
      "epoch: 363 loss: 0.05774780362844467\n",
      "epoch: 364 loss: 0.057388126850128174\n",
      "epoch: 365 loss: 0.057032570242881775\n",
      "epoch: 366 loss: 0.05668072775006294\n",
      "epoch: 367 loss: 0.05633196607232094\n",
      "epoch: 368 loss: 0.055986858904361725\n",
      "epoch: 369 loss: 0.05564585700631142\n",
      "epoch: 370 loss: 0.05530795827507973\n",
      "epoch: 371 loss: 0.05497374385595322\n",
      "epoch: 372 loss: 0.05464321747422218\n",
      "epoch: 373 loss: 0.05431600660085678\n",
      "epoch: 374 loss: 0.05399257689714432\n",
      "epoch: 375 loss: 0.0536724291741848\n",
      "epoch: 376 loss: 0.05335560068488121\n",
      "epoch: 377 loss: 0.05304209887981415\n",
      "epoch: 378 loss: 0.05273154750466347\n",
      "epoch: 379 loss: 0.052424248307943344\n",
      "epoch: 380 loss: 0.05212023854255676\n",
      "epoch: 381 loss: 0.05181926116347313\n",
      "epoch: 382 loss: 0.05152130872011185\n",
      "epoch: 383 loss: 0.05122574418783188\n",
      "epoch: 384 loss: 0.050933487713336945\n",
      "epoch: 385 loss: 0.05064413696527481\n",
      "epoch: 386 loss: 0.05035771057009697\n",
      "epoch: 387 loss: 0.0500740110874176\n",
      "epoch: 388 loss: 0.0497930608689785\n",
      "epoch: 389 loss: 0.04951474070549011\n",
      "epoch: 390 loss: 0.04923955351114273\n",
      "epoch: 391 loss: 0.04896660894155502\n",
      "epoch: 392 loss: 0.04869610816240311\n",
      "epoch: 393 loss: 0.04842808097600937\n",
      "epoch: 394 loss: 0.04816276580095291\n",
      "epoch: 395 loss: 0.04790012538433075\n",
      "epoch: 396 loss: 0.04763932526111603\n",
      "epoch: 397 loss: 0.047380998730659485\n",
      "epoch: 398 loss: 0.04712548851966858\n",
      "epoch: 399 loss: 0.046872109174728394\n",
      "epoch: 400 loss: 0.046620868146419525\n",
      "epoch: 401 loss: 0.046371955424547195\n",
      "epoch: 402 loss: 0.04612570255994797\n",
      "epoch: 403 loss: 0.045881807804107666\n",
      "epoch: 404 loss: 0.04564033821225166\n",
      "epoch: 405 loss: 0.045400992035865784\n",
      "epoch: 406 loss: 0.045163799077272415\n",
      "epoch: 407 loss: 0.044928520917892456\n",
      "epoch: 408 loss: 0.04469534009695053\n",
      "epoch: 409 loss: 0.0444648414850235\n",
      "epoch: 410 loss: 0.044236667454242706\n",
      "epoch: 411 loss: 0.04401005059480667\n",
      "epoch: 412 loss: 0.043785735964775085\n",
      "epoch: 413 loss: 0.04356348514556885\n",
      "epoch: 414 loss: 0.04334314167499542\n",
      "epoch: 415 loss: 0.04312492907047272\n",
      "epoch: 416 loss: 0.042908504605293274\n",
      "epoch: 417 loss: 0.04269390180706978\n",
      "epoch: 418 loss: 0.042481157928705215\n",
      "epoch: 419 loss: 0.042270440608263016\n",
      "epoch: 420 loss: 0.04206176847219467\n",
      "epoch: 421 loss: 0.04185493290424347\n",
      "epoch: 422 loss: 0.04164987429976463\n",
      "epoch: 423 loss: 0.04144652932882309\n",
      "epoch: 424 loss: 0.041244953870773315\n",
      "epoch: 425 loss: 0.04104507714509964\n",
      "epoch: 426 loss: 0.04084758087992668\n",
      "epoch: 427 loss: 0.040651462972164154\n",
      "epoch: 428 loss: 0.040456950664520264\n",
      "epoch: 429 loss: 0.0402638241648674\n",
      "epoch: 430 loss: 0.040072545409202576\n",
      "epoch: 431 loss: 0.03988323360681534\n",
      "epoch: 432 loss: 0.03969559818506241\n",
      "epoch: 433 loss: 0.039509277790784836\n",
      "epoch: 434 loss: 0.039324574172496796\n",
      "epoch: 435 loss: 0.03914140164852142\n",
      "epoch: 436 loss: 0.038960136473178864\n",
      "epoch: 437 loss: 0.03878003731369972\n",
      "epoch: 438 loss: 0.03860151022672653\n",
      "epoch: 439 loss: 0.03842442110180855\n",
      "epoch: 440 loss: 0.03824898600578308\n",
      "epoch: 441 loss: 0.038075514137744904\n",
      "epoch: 442 loss: 0.03790319710969925\n",
      "epoch: 443 loss: 0.03773193806409836\n",
      "epoch: 444 loss: 0.03756234049797058\n",
      "epoch: 445 loss: 0.03739427030086517\n",
      "epoch: 446 loss: 0.03722767531871796\n",
      "epoch: 447 loss: 0.037061966955661774\n",
      "epoch: 448 loss: 0.036898091435432434\n",
      "epoch: 449 loss: 0.03673548996448517\n",
      "epoch: 450 loss: 0.03657420724630356\n",
      "epoch: 451 loss: 0.036414019763469696\n",
      "epoch: 452 loss: 0.036255355924367905\n",
      "epoch: 453 loss: 0.036097899079322815\n",
      "epoch: 454 loss: 0.03594156354665756\n",
      "epoch: 455 loss: 0.035786695778369904\n",
      "epoch: 456 loss: 0.03563312441110611\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 457 loss: 0.035480767488479614\n",
      "epoch: 458 loss: 0.0353296734392643\n",
      "epoch: 459 loss: 0.03517981618642807\n",
      "epoch: 460 loss: 0.03503115847706795\n",
      "epoch: 461 loss: 0.03488367423415184\n",
      "epoch: 462 loss: 0.03473738580942154\n",
      "epoch: 463 loss: 0.03459247946739197\n",
      "epoch: 464 loss: 0.03444845229387283\n",
      "epoch: 465 loss: 0.03430569916963577\n",
      "epoch: 466 loss: 0.03416384011507034\n",
      "epoch: 467 loss: 0.03402356803417206\n",
      "epoch: 468 loss: 0.0338841937482357\n",
      "epoch: 469 loss: 0.03374551981687546\n",
      "epoch: 470 loss: 0.033608149737119675\n",
      "epoch: 471 loss: 0.033471930772066116\n",
      "epoch: 472 loss: 0.03333700820803642\n",
      "epoch: 473 loss: 0.03320275992155075\n",
      "epoch: 474 loss: 0.033070001751184464\n",
      "epoch: 475 loss: 0.03293789178133011\n",
      "epoch: 476 loss: 0.03280674293637276\n",
      "epoch: 477 loss: 0.03267660737037659\n",
      "epoch: 478 loss: 0.032547738403081894\n",
      "epoch: 479 loss: 0.03241991624236107\n",
      "epoch: 480 loss: 0.032292690128088\n",
      "epoch: 481 loss: 0.03216669335961342\n",
      "epoch: 482 loss: 0.03204142674803734\n",
      "epoch: 483 loss: 0.03191746398806572\n",
      "epoch: 484 loss: 0.031794607639312744\n",
      "epoch: 485 loss: 0.0316721610724926\n",
      "epoch: 486 loss: 0.03155025094747543\n",
      "epoch: 487 loss: 0.03142974525690079\n",
      "epoch: 488 loss: 0.03131033852696419\n",
      "epoch: 489 loss: 0.031191524118185043\n",
      "epoch: 490 loss: 0.031073814257979393\n",
      "epoch: 491 loss: 0.030957071110606194\n",
      "epoch: 492 loss: 0.030841179192066193\n",
      "epoch: 493 loss: 0.030726533383131027\n",
      "epoch: 494 loss: 0.03061254508793354\n",
      "epoch: 495 loss: 0.030498826876282692\n",
      "epoch: 496 loss: 0.03038610704243183\n",
      "epoch: 497 loss: 0.030273960903286934\n",
      "epoch: 498 loss: 0.0301628727465868\n",
      "epoch: 499 loss: 0.030052803456783295\n",
      "tensor([[ 2.9545,  7.4859,  1.3939,  ..., -5.7291, -5.6919, -5.7321],\n",
      "        [ 3.6126,  6.0274,  0.7367,  ..., -4.2124, -3.9982, -4.1195],\n",
      "        [ 5.4686,  4.6176,  2.2869,  ..., -4.2781, -4.1335, -4.1915],\n",
      "        ...,\n",
      "        [ 3.8966,  2.3296,  4.8495,  ..., -3.3367, -3.4135, -3.3150],\n",
      "        [ 9.5333,  3.3210,  3.3614,  ..., -5.0833, -4.9253, -4.9880],\n",
      "        [10.1118,  4.6168,  4.3653,  ..., -6.1587, -6.0154, -6.0658]],\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, in_c, hid_c, out_c):\n",
    "        super(GCN,self).__init__()    # 构造函数\n",
    "\n",
    "        self.conv1 = GCNConv(in_channels=in_c, out_channels=hid_c)\n",
    "        self.conv2 = GCNConv(in_channels=hid_c, out_channels=out_c)\n",
    "  \n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index    # x:节点特征矩阵; edge_index:COO格式的图形连接，维度[2,边的数量]，数据：[ [源节点],[目标节点] ]\n",
    "        \n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.conv2(x, edge_index)\n",
    "        \n",
    "        x1 = F.log_softmax(x, dim=1)\n",
    "        \n",
    "        return x1, x\n",
    "\n",
    "model = GCN(in_c=dataset.num_node_features,hid_c=100,out_c=dataset.num_node_features)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n",
    "\n",
    "print(model(data))\n",
    "\n",
    "model.train()\n",
    "for epoch in range(500):\n",
    "    optimizer.zero_grad()\n",
    "    out1, out = model(data)\n",
    "    # 交叉熵损失\n",
    "    loss = F.nll_loss(out1[data.train_mask], data.y[data.train_mask])       # 负对数似然。\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print('epoch:',epoch, 'loss:',loss.item())  \n",
    "\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "440a03ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calinski-Harabaz Index： 34.85395874702531\n",
      "Silhouette Coefficient： 0.4443395\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn import cluster\n",
    "from sklearn import metrics\n",
    "\n",
    "out=out.cpu().detach().numpy()\n",
    "model = cluster.KMeans(n_clusters=4)\n",
    "model.fit(out)\n",
    "\n",
    "y_predict = model.predict(out)\n",
    "\n",
    "# Calinski-Harabaz Index：越大越好\n",
    "# Silhouette Coefficient：轮廓系数（越大越好)\n",
    "print('Calinski-Harabaz Index：',metrics.calinski_harabasz_score(out,y_predict))\n",
    "print('Silhouette Coefficient：',metrics.silhouette_score(out,y_predict))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5bce02e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>221.331146</td>\n",
       "      <td>-72.517181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-62.793858</td>\n",
       "      <td>176.965225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>168.154343</td>\n",
       "      <td>-17.605772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>120.997375</td>\n",
       "      <td>24.414715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-76.015686</td>\n",
       "      <td>-12.484374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-149.220886</td>\n",
       "      <td>-114.855545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-67.272942</td>\n",
       "      <td>52.500839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-21.159119</td>\n",
       "      <td>-108.043968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-22.462019</td>\n",
       "      <td>106.347786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>39.575966</td>\n",
       "      <td>-66.707489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-29.648930</td>\n",
       "      <td>11.811772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14.501960</td>\n",
       "      <td>166.065613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-79.864883</td>\n",
       "      <td>-146.907608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>63.507591</td>\n",
       "      <td>37.158001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>85.868721</td>\n",
       "      <td>-159.303955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-123.391037</td>\n",
       "      <td>22.704531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-89.419754</td>\n",
       "      <td>104.265083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-12.040999</td>\n",
       "      <td>-177.268723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>104.047142</td>\n",
       "      <td>-90.539070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-118.483734</td>\n",
       "      <td>-52.865055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-15.876863</td>\n",
       "      <td>-46.957256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>33.094486</td>\n",
       "      <td>-125.607750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>89.912590</td>\n",
       "      <td>150.364044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-151.952011</td>\n",
       "      <td>98.984215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-187.882980</td>\n",
       "      <td>35.569088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>43.701237</td>\n",
       "      <td>100.446030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-67.813118</td>\n",
       "      <td>-77.436234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>87.468719</td>\n",
       "      <td>-28.077471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-178.366989</td>\n",
       "      <td>-36.215321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>28.734932</td>\n",
       "      <td>-6.726018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>112.268105</td>\n",
       "      <td>86.474052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.440912</td>\n",
       "      <td>55.061329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>204.968155</td>\n",
       "      <td>57.578705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>253.835968</td>\n",
       "      <td>1.636227</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0           1\n",
       "2  221.331146  -72.517181\n",
       "2  -62.793858  176.965225\n",
       "2  168.154343  -17.605772\n",
       "3  120.997375   24.414715\n",
       "0  -76.015686  -12.484374\n",
       "0 -149.220886 -114.855545\n",
       "0  -67.272942   52.500839\n",
       "1  -21.159119 -108.043968\n",
       "1  -22.462019  106.347786\n",
       "1   39.575966  -66.707489\n",
       "0  -29.648930   11.811772\n",
       "1   14.501960  166.065613\n",
       "1  -79.864883 -146.907608\n",
       "1   63.507591   37.158001\n",
       "1   85.868721 -159.303955\n",
       "1 -123.391037   22.704531\n",
       "0  -89.419754  104.265083\n",
       "1  -12.040999 -177.268723\n",
       "1  104.047142  -90.539070\n",
       "1 -118.483734  -52.865055\n",
       "1  -15.876863  -46.957256\n",
       "1   33.094486 -125.607750\n",
       "1   89.912590  150.364044\n",
       "3 -151.952011   98.984215\n",
       "3 -187.882980   35.569088\n",
       "3   43.701237  100.446030\n",
       "1  -67.813118  -77.436234\n",
       "3   87.468719  -28.077471\n",
       "1 -178.366989  -36.215321\n",
       "3   28.734932   -6.726018\n",
       "1  112.268105   86.474052\n",
       "3    7.440912   55.061329\n",
       "2  204.968155   57.578705\n",
       "2  253.835968    1.636227"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "t_sne = TSNE()\n",
    "data=t_sne.fit_transform(out)\n",
    "data=pd.DataFrame(data)\n",
    "data = pd.DataFrame(data,index=y_predict)\n",
    "data_tsne = pd.DataFrame(t_sne.embedding_, index =y_predict)\n",
    "data_tsne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f69d9ffb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f001e192e80>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "d = data_tsne[data_tsne.index == 0]     #找出聚类类别为0的数据对应的降维结果\n",
    "plt.scatter(d[0], d[1],c='lightgreen',marker='o')\n",
    "d = data_tsne[data_tsne.index == 1]\n",
    "plt.scatter(d[0], d[1], c='orange',\tmarker='o')\n",
    "d = data_tsne[data_tsne.index == 2]\n",
    "plt.scatter(d[0], d[1], c='lightblue',marker='o')\n",
    "d = data_tsne[data_tsne.index == 3]\n",
    "plt.scatter(d[0], d[1], c='red',marker='o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4dac16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
