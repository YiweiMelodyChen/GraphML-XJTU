{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "385d5f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset: KarateClub():\n",
      "======================\n",
      "Number of graphs: 1\n",
      "Number of features: 34\n",
      "Number of classes: 4\n",
      "\n",
      "Data(x=[34, 34], edge_index=[2, 156], y=[34], train_mask=[34])\n",
      "======================\n",
      "Number of nodes: 34\n",
      "Number of edges: 156\n",
      "Average node degree: 4.59\n",
      "Number of training nodes: 4\n",
      "Training node label rate: 0.12\n",
      "Contains isolated nodes: False\n",
      "Contains self-loops: False\n",
      "Is undirected: True\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.datasets import NELL\n",
    "from torch_geometric.datasets import KarateClub\n",
    "from torch_geometric.transforms import NormalizeFeatures\n",
    "from torch_geometric.nn import GCNConv, SAGEConv, GATConv,RGCNConv\n",
    "import torch\n",
    "\n",
    "dataset = KarateClub()\n",
    "\n",
    "print()\n",
    "print(f'Dataset: {dataset}:')\n",
    "print('======================')\n",
    "print(f'Number of graphs: {len(dataset)}')\n",
    "print(f'Number of features: {dataset.num_features}')\n",
    "print(f'Number of classes: {dataset.num_classes}')\n",
    "data = dataset[0]\n",
    "# data = dataset[0]  # Get the first graph object.\n",
    "\n",
    "print()\n",
    "print(data)\n",
    "print('======================')\n",
    "\n",
    "# Gather some statistics about the graph.\n",
    "print(f'Number of nodes: {data.num_nodes}')\n",
    "print(f'Number of edges: {data.num_edges}')\n",
    "print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
    "print(f'Number of training nodes: {data.train_mask.sum()}')\n",
    "print(f'Training node label rate: {int(data.train_mask.sum()) / data.num_nodes:.2f}')\n",
    "print(f'Contains isolated nodes: {data.has_isolated_nodes()}')\n",
    "print(f'Contains self-loops: {data.has_self_loops()}')\n",
    "print(f'Is undirected: {data.is_undirected()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53fbb77a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[-3.5097, -3.5412, -3.6191,  ..., -3.4946, -3.4863, -3.5466],\n",
      "        [-3.5053, -3.5482, -3.5715,  ..., -3.5225, -3.5200, -3.5611],\n",
      "        [-3.5503, -3.5487, -3.5660,  ..., -3.5294, -3.5178, -3.5211],\n",
      "        ...,\n",
      "        [-3.5359, -3.5390, -3.5538,  ..., -3.5110, -3.4844, -3.5701],\n",
      "        [-3.5832, -3.5174, -3.5633,  ..., -3.5031, -3.4386, -3.6251],\n",
      "        [-3.5926, -3.5290, -3.5713,  ..., -3.5022, -3.4554, -3.6038]],\n",
      "       grad_fn=<LogSoftmaxBackward>), tensor([[ 0.0304, -0.0011, -0.0790,  ...,  0.0455,  0.0538, -0.0065],\n",
      "        [ 0.0284, -0.0146, -0.0379,  ...,  0.0112,  0.0136, -0.0275],\n",
      "        [-0.0187, -0.0171, -0.0344,  ...,  0.0021,  0.0138,  0.0105],\n",
      "        ...,\n",
      "        [-0.0020, -0.0051, -0.0198,  ...,  0.0229,  0.0495, -0.0362],\n",
      "        [-0.0421,  0.0237, -0.0222,  ...,  0.0379,  0.1025, -0.0840],\n",
      "        [-0.0535,  0.0101, -0.0322,  ...,  0.0370,  0.0838, -0.0647]],\n",
      "       grad_fn=<AddBackward0>))\n",
      "epoch: 0 loss: 3.544055223464966\n",
      "epoch: 1 loss: 3.531582832336426\n",
      "epoch: 2 loss: 3.5192723274230957\n",
      "epoch: 3 loss: 3.507038116455078\n",
      "epoch: 4 loss: 3.494865894317627\n",
      "epoch: 5 loss: 3.4827380180358887\n",
      "epoch: 6 loss: 3.47066593170166\n",
      "epoch: 7 loss: 3.4585726261138916\n",
      "epoch: 8 loss: 3.4464495182037354\n",
      "epoch: 9 loss: 3.4343008995056152\n",
      "epoch: 10 loss: 3.422097682952881\n",
      "epoch: 11 loss: 3.409794569015503\n",
      "epoch: 12 loss: 3.397322416305542\n",
      "epoch: 13 loss: 3.38466215133667\n",
      "epoch: 14 loss: 3.371788501739502\n",
      "epoch: 15 loss: 3.3587470054626465\n",
      "epoch: 16 loss: 3.3454389572143555\n",
      "epoch: 17 loss: 3.3318939208984375\n",
      "epoch: 18 loss: 3.318002223968506\n",
      "epoch: 19 loss: 3.3038008213043213\n",
      "epoch: 20 loss: 3.289283037185669\n",
      "epoch: 21 loss: 3.2744338512420654\n",
      "epoch: 22 loss: 3.2592434883117676\n",
      "epoch: 23 loss: 3.2436325550079346\n",
      "epoch: 24 loss: 3.227560520172119\n",
      "epoch: 25 loss: 3.2110464572906494\n",
      "epoch: 26 loss: 3.1940243244171143\n",
      "epoch: 27 loss: 3.1765077114105225\n",
      "epoch: 28 loss: 3.158444881439209\n",
      "epoch: 29 loss: 3.139810085296631\n",
      "epoch: 30 loss: 3.1206367015838623\n",
      "epoch: 31 loss: 3.1008944511413574\n",
      "epoch: 32 loss: 3.0805530548095703\n",
      "epoch: 33 loss: 3.0595803260803223\n",
      "epoch: 34 loss: 3.037984848022461\n",
      "epoch: 35 loss: 3.015699625015259\n",
      "epoch: 36 loss: 2.9927890300750732\n",
      "epoch: 37 loss: 2.9692721366882324\n",
      "epoch: 38 loss: 2.9451067447662354\n",
      "epoch: 39 loss: 2.9203126430511475\n",
      "epoch: 40 loss: 2.8948588371276855\n",
      "epoch: 41 loss: 2.8687658309936523\n",
      "epoch: 42 loss: 2.842040777206421\n",
      "epoch: 43 loss: 2.814694404602051\n",
      "epoch: 44 loss: 2.786707639694214\n",
      "epoch: 45 loss: 2.7581019401550293\n",
      "epoch: 46 loss: 2.7289445400238037\n",
      "epoch: 47 loss: 2.699226140975952\n",
      "epoch: 48 loss: 2.6689260005950928\n",
      "epoch: 49 loss: 2.6381006240844727\n",
      "epoch: 50 loss: 2.6067073345184326\n",
      "epoch: 51 loss: 2.5747439861297607\n",
      "epoch: 52 loss: 2.542302370071411\n",
      "epoch: 53 loss: 2.5093777179718018\n",
      "epoch: 54 loss: 2.476043939590454\n",
      "epoch: 55 loss: 2.442326307296753\n",
      "epoch: 56 loss: 2.4082882404327393\n",
      "epoch: 57 loss: 2.3739962577819824\n",
      "epoch: 58 loss: 2.339488983154297\n",
      "epoch: 59 loss: 2.304772138595581\n",
      "epoch: 60 loss: 2.2699525356292725\n",
      "epoch: 61 loss: 2.235085964202881\n",
      "epoch: 62 loss: 2.200209856033325\n",
      "epoch: 63 loss: 2.1653900146484375\n",
      "epoch: 64 loss: 2.130662441253662\n",
      "epoch: 65 loss: 2.0960869789123535\n",
      "epoch: 66 loss: 2.0616908073425293\n",
      "epoch: 67 loss: 2.027510404586792\n",
      "epoch: 68 loss: 1.993577480316162\n",
      "epoch: 69 loss: 1.9599671363830566\n",
      "epoch: 70 loss: 1.9267245531082153\n",
      "epoch: 71 loss: 1.8938757181167603\n",
      "epoch: 72 loss: 1.861449956893921\n",
      "epoch: 73 loss: 1.8294814825057983\n",
      "epoch: 74 loss: 1.7979964017868042\n",
      "epoch: 75 loss: 1.767040491104126\n",
      "epoch: 76 loss: 1.7366269826889038\n",
      "epoch: 77 loss: 1.7067615985870361\n",
      "epoch: 78 loss: 1.6774604320526123\n",
      "epoch: 79 loss: 1.6487343311309814\n",
      "epoch: 80 loss: 1.6205823421478271\n",
      "epoch: 81 loss: 1.5930073261260986\n",
      "epoch: 82 loss: 1.5660085678100586\n",
      "epoch: 83 loss: 1.5395917892456055\n",
      "epoch: 84 loss: 1.5137455463409424\n",
      "epoch: 85 loss: 1.4884477853775024\n",
      "epoch: 86 loss: 1.4637137651443481\n",
      "epoch: 87 loss: 1.4395684003829956\n",
      "epoch: 88 loss: 1.415975570678711\n",
      "epoch: 89 loss: 1.392916202545166\n",
      "epoch: 90 loss: 1.37039315700531\n",
      "epoch: 91 loss: 1.348390817642212\n",
      "epoch: 92 loss: 1.3268365859985352\n",
      "epoch: 93 loss: 1.3057498931884766\n",
      "epoch: 94 loss: 1.2851307392120361\n",
      "epoch: 95 loss: 1.2649590969085693\n",
      "epoch: 96 loss: 1.2451993227005005\n",
      "epoch: 97 loss: 1.2258621454238892\n",
      "epoch: 98 loss: 1.2069330215454102\n",
      "epoch: 99 loss: 1.1884090900421143\n",
      "epoch: 100 loss: 1.1702463626861572\n",
      "epoch: 101 loss: 1.152430772781372\n",
      "epoch: 102 loss: 1.1349689960479736\n",
      "epoch: 103 loss: 1.1178406476974487\n",
      "epoch: 104 loss: 1.1010255813598633\n",
      "epoch: 105 loss: 1.084526777267456\n",
      "epoch: 106 loss: 1.0683289766311646\n",
      "epoch: 107 loss: 1.05241858959198\n",
      "epoch: 108 loss: 1.0368032455444336\n",
      "epoch: 109 loss: 1.0214719772338867\n",
      "epoch: 110 loss: 1.0064033269882202\n",
      "epoch: 111 loss: 0.9915899038314819\n",
      "epoch: 112 loss: 0.9770315885543823\n",
      "epoch: 113 loss: 0.9627096652984619\n",
      "epoch: 114 loss: 0.9486039280891418\n",
      "epoch: 115 loss: 0.9347214698791504\n",
      "epoch: 116 loss: 0.9210545420646667\n",
      "epoch: 117 loss: 0.9076016545295715\n",
      "epoch: 118 loss: 0.8943572640419006\n",
      "epoch: 119 loss: 0.8813215494155884\n",
      "epoch: 120 loss: 0.8684788942337036\n",
      "epoch: 121 loss: 0.8558412194252014\n",
      "epoch: 122 loss: 0.8433993458747864\n",
      "epoch: 123 loss: 0.8311472535133362\n",
      "epoch: 124 loss: 0.8190740346908569\n",
      "epoch: 125 loss: 0.8071725368499756\n",
      "epoch: 126 loss: 0.7954373359680176\n",
      "epoch: 127 loss: 0.7838736176490784\n",
      "epoch: 128 loss: 0.7724846005439758\n",
      "epoch: 129 loss: 0.7612597942352295\n",
      "epoch: 130 loss: 0.7501983046531677\n",
      "epoch: 131 loss: 0.7392930388450623\n",
      "epoch: 132 loss: 0.7285479307174683\n",
      "epoch: 133 loss: 0.7179673910140991\n",
      "epoch: 134 loss: 0.7075408101081848\n",
      "epoch: 135 loss: 0.6972647905349731\n",
      "epoch: 136 loss: 0.6871311068534851\n",
      "epoch: 137 loss: 0.6771377325057983\n",
      "epoch: 138 loss: 0.6672969460487366\n",
      "epoch: 139 loss: 0.6576011776924133\n",
      "epoch: 140 loss: 0.6480416655540466\n",
      "epoch: 141 loss: 0.6386145353317261\n",
      "epoch: 142 loss: 0.629327654838562\n",
      "epoch: 143 loss: 0.620171308517456\n",
      "epoch: 144 loss: 0.611146092414856\n",
      "epoch: 145 loss: 0.6022486090660095\n",
      "epoch: 146 loss: 0.5934826135635376\n",
      "epoch: 147 loss: 0.5848414301872253\n",
      "epoch: 148 loss: 0.5763212442398071\n",
      "epoch: 149 loss: 0.5679177641868591\n",
      "epoch: 150 loss: 0.5596321821212769\n",
      "epoch: 151 loss: 0.5514693856239319\n",
      "epoch: 152 loss: 0.5434175133705139\n",
      "epoch: 153 loss: 0.5354808568954468\n",
      "epoch: 154 loss: 0.5276493430137634\n",
      "epoch: 155 loss: 0.5199257731437683\n",
      "epoch: 156 loss: 0.5123164653778076\n",
      "epoch: 157 loss: 0.5048275589942932\n",
      "epoch: 158 loss: 0.4974520206451416\n",
      "epoch: 159 loss: 0.49019116163253784\n",
      "epoch: 160 loss: 0.4830395579338074\n",
      "epoch: 161 loss: 0.47598880529403687\n",
      "epoch: 162 loss: 0.4690416157245636\n",
      "epoch: 163 loss: 0.46220070123672485\n",
      "epoch: 164 loss: 0.45546072721481323\n",
      "epoch: 165 loss: 0.44882190227508545\n",
      "epoch: 166 loss: 0.442288339138031\n",
      "epoch: 167 loss: 0.43585678935050964\n",
      "epoch: 168 loss: 0.4295209050178528\n",
      "epoch: 169 loss: 0.42327985167503357\n",
      "epoch: 170 loss: 0.41713061928749084\n",
      "epoch: 171 loss: 0.4110780358314514\n",
      "epoch: 172 loss: 0.4051154851913452\n",
      "epoch: 173 loss: 0.39925312995910645\n",
      "epoch: 174 loss: 0.3934827148914337\n",
      "epoch: 175 loss: 0.3878014385700226\n",
      "epoch: 176 loss: 0.3822113275527954\n",
      "epoch: 177 loss: 0.37670502066612244\n",
      "epoch: 178 loss: 0.3712819814682007\n",
      "epoch: 179 loss: 0.36594027280807495\n",
      "epoch: 180 loss: 0.3606799244880676\n",
      "epoch: 181 loss: 0.35550445318222046\n",
      "epoch: 182 loss: 0.3504074811935425\n",
      "epoch: 183 loss: 0.3453879952430725\n",
      "epoch: 184 loss: 0.34044137597084045\n",
      "epoch: 185 loss: 0.3355753719806671\n",
      "epoch: 186 loss: 0.33078619837760925\n",
      "epoch: 187 loss: 0.32606521248817444\n",
      "epoch: 188 loss: 0.32141757011413574\n",
      "epoch: 189 loss: 0.31684771180152893\n",
      "epoch: 190 loss: 0.3123505711555481\n",
      "epoch: 191 loss: 0.30792349576950073\n",
      "epoch: 192 loss: 0.30356770753860474\n",
      "epoch: 193 loss: 0.2992803752422333\n",
      "epoch: 194 loss: 0.29506009817123413\n",
      "epoch: 195 loss: 0.2909078299999237\n",
      "epoch: 196 loss: 0.28681811690330505\n",
      "epoch: 197 loss: 0.28279387950897217\n",
      "epoch: 198 loss: 0.27883368730545044\n",
      "epoch: 199 loss: 0.2749367654323578\n",
      "epoch: 200 loss: 0.2711022198200226\n",
      "epoch: 201 loss: 0.26733431220054626\n",
      "epoch: 202 loss: 0.2636275887489319\n",
      "epoch: 203 loss: 0.25998303294181824\n",
      "epoch: 204 loss: 0.25639915466308594\n",
      "epoch: 205 loss: 0.2528720498085022\n",
      "epoch: 206 loss: 0.24941155314445496\n",
      "epoch: 207 loss: 0.24601146578788757\n",
      "epoch: 208 loss: 0.24266499280929565\n",
      "epoch: 209 loss: 0.239374577999115\n",
      "epoch: 210 loss: 0.23614025115966797\n",
      "epoch: 211 loss: 0.23295453190803528\n",
      "epoch: 212 loss: 0.22982637584209442\n",
      "epoch: 213 loss: 0.2267555147409439\n",
      "epoch: 214 loss: 0.22373393177986145\n",
      "epoch: 215 loss: 0.22076094150543213\n",
      "epoch: 216 loss: 0.2178388237953186\n",
      "epoch: 217 loss: 0.21497204899787903\n",
      "epoch: 218 loss: 0.21215805411338806\n",
      "epoch: 219 loss: 0.2093888521194458\n",
      "epoch: 220 loss: 0.2066650241613388\n",
      "epoch: 221 loss: 0.20398886501789093\n",
      "epoch: 222 loss: 0.20135682821273804\n",
      "epoch: 223 loss: 0.19876761734485626\n",
      "epoch: 224 loss: 0.19622600078582764\n",
      "epoch: 225 loss: 0.19372200965881348\n",
      "epoch: 226 loss: 0.19125743210315704\n",
      "epoch: 227 loss: 0.18883633613586426\n",
      "epoch: 228 loss: 0.18645638227462769\n",
      "epoch: 229 loss: 0.18411853909492493\n",
      "epoch: 230 loss: 0.18181943893432617\n",
      "epoch: 231 loss: 0.17955708503723145\n",
      "epoch: 232 loss: 0.1773337721824646\n",
      "epoch: 233 loss: 0.17514671385288239\n",
      "epoch: 234 loss: 0.17299692332744598\n",
      "epoch: 235 loss: 0.170882910490036\n",
      "epoch: 236 loss: 0.16880342364311218\n",
      "epoch: 237 loss: 0.1667570024728775\n",
      "epoch: 238 loss: 0.16474469006061554\n",
      "epoch: 239 loss: 0.16276666522026062\n",
      "epoch: 240 loss: 0.16082268953323364\n",
      "epoch: 241 loss: 0.1589137315750122\n",
      "epoch: 242 loss: 0.1570347547531128\n",
      "epoch: 243 loss: 0.15518692135810852\n",
      "epoch: 244 loss: 0.1533670574426651\n",
      "epoch: 245 loss: 0.15157832205295563\n",
      "epoch: 246 loss: 0.1498190015554428\n",
      "epoch: 247 loss: 0.14808784425258636\n",
      "epoch: 248 loss: 0.14638404548168182\n",
      "epoch: 249 loss: 0.1447090059518814\n",
      "epoch: 250 loss: 0.1430625021457672\n",
      "epoch: 251 loss: 0.14144371449947357\n",
      "epoch: 252 loss: 0.13985060155391693\n",
      "epoch: 253 loss: 0.13828454911708832\n",
      "epoch: 254 loss: 0.13674235343933105\n",
      "epoch: 255 loss: 0.13522540032863617\n",
      "epoch: 256 loss: 0.13373303413391113\n",
      "epoch: 257 loss: 0.1322648525238037\n",
      "epoch: 258 loss: 0.13081806898117065\n",
      "epoch: 259 loss: 0.12939445674419403\n",
      "epoch: 260 loss: 0.12799416482448578\n",
      "epoch: 261 loss: 0.1266152262687683\n",
      "epoch: 262 loss: 0.1252572387456894\n",
      "epoch: 263 loss: 0.12392003834247589\n",
      "epoch: 264 loss: 0.12260384112596512\n",
      "epoch: 265 loss: 0.1213085874915123\n",
      "epoch: 266 loss: 0.12003341317176819\n",
      "epoch: 267 loss: 0.11877724528312683\n",
      "epoch: 268 loss: 0.11753979325294495\n",
      "epoch: 269 loss: 0.11632252484560013\n",
      "epoch: 270 loss: 0.11512458324432373\n",
      "epoch: 271 loss: 0.11394313722848892\n",
      "epoch: 272 loss: 0.1127798855304718\n",
      "epoch: 273 loss: 0.11163447052240372\n",
      "epoch: 274 loss: 0.11050654947757721\n",
      "epoch: 275 loss: 0.1093950867652893\n",
      "epoch: 276 loss: 0.1083013117313385\n",
      "epoch: 277 loss: 0.10722432285547256\n",
      "epoch: 278 loss: 0.106163389980793\n",
      "epoch: 279 loss: 0.10511758923530579\n",
      "epoch: 280 loss: 0.10408847779035568\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 281 loss: 0.1030738353729248\n",
      "epoch: 282 loss: 0.10207310318946838\n",
      "epoch: 283 loss: 0.10108567029237747\n",
      "epoch: 284 loss: 0.1001136526465416\n",
      "epoch: 285 loss: 0.09915568679571152\n",
      "epoch: 286 loss: 0.09821117669343948\n",
      "epoch: 287 loss: 0.09728024899959564\n",
      "epoch: 288 loss: 0.09636280685663223\n",
      "epoch: 289 loss: 0.09545814990997314\n",
      "epoch: 290 loss: 0.09456592053174973\n",
      "epoch: 291 loss: 0.09368658810853958\n",
      "epoch: 292 loss: 0.09282048046588898\n",
      "epoch: 293 loss: 0.09196650981903076\n",
      "epoch: 294 loss: 0.09112459421157837\n",
      "epoch: 295 loss: 0.0902942642569542\n",
      "epoch: 296 loss: 0.08947457373142242\n",
      "epoch: 297 loss: 0.08866658061742783\n",
      "epoch: 298 loss: 0.08786997944116592\n",
      "epoch: 299 loss: 0.08708494156599045\n",
      "epoch: 300 loss: 0.0863097533583641\n",
      "epoch: 301 loss: 0.08554637432098389\n",
      "epoch: 302 loss: 0.08479408919811249\n",
      "epoch: 303 loss: 0.08405192196369171\n",
      "epoch: 304 loss: 0.08331946283578873\n",
      "epoch: 305 loss: 0.08259709179401398\n",
      "epoch: 306 loss: 0.08188417553901672\n",
      "epoch: 307 loss: 0.08118164539337158\n",
      "epoch: 308 loss: 0.08048847317695618\n",
      "epoch: 309 loss: 0.07980362325906754\n",
      "epoch: 310 loss: 0.07912833988666534\n",
      "epoch: 311 loss: 0.0784633681178093\n",
      "epoch: 312 loss: 0.07780653238296509\n",
      "epoch: 313 loss: 0.07715717703104019\n",
      "epoch: 314 loss: 0.07651729136705399\n",
      "epoch: 315 loss: 0.07588540762662888\n",
      "epoch: 316 loss: 0.07526227086782455\n",
      "epoch: 317 loss: 0.07464668154716492\n",
      "epoch: 318 loss: 0.07403846085071564\n",
      "epoch: 319 loss: 0.07343967258930206\n",
      "epoch: 320 loss: 0.07284747809171677\n",
      "epoch: 321 loss: 0.07226287573575974\n",
      "epoch: 322 loss: 0.07168611884117126\n",
      "epoch: 323 loss: 0.07111674547195435\n",
      "epoch: 324 loss: 0.07055478543043137\n",
      "epoch: 325 loss: 0.06999962031841278\n",
      "epoch: 326 loss: 0.06945135444402695\n",
      "epoch: 327 loss: 0.06891032308340073\n",
      "epoch: 328 loss: 0.06837581843137741\n",
      "epoch: 329 loss: 0.06784792244434357\n",
      "epoch: 330 loss: 0.0673268660902977\n",
      "epoch: 331 loss: 0.06681188941001892\n",
      "epoch: 332 loss: 0.06630322337150574\n",
      "epoch: 333 loss: 0.06580107659101486\n",
      "epoch: 334 loss: 0.0653049424290657\n",
      "epoch: 335 loss: 0.06481477618217468\n",
      "epoch: 336 loss: 0.06433141231536865\n",
      "epoch: 337 loss: 0.06385311484336853\n",
      "epoch: 338 loss: 0.06338091939687729\n",
      "epoch: 339 loss: 0.06291534006595612\n",
      "epoch: 340 loss: 0.062454693019390106\n",
      "epoch: 341 loss: 0.06199975311756134\n",
      "epoch: 342 loss: 0.061550188809633255\n",
      "epoch: 343 loss: 0.06110605597496033\n",
      "epoch: 344 loss: 0.060666345059871674\n",
      "epoch: 345 loss: 0.06023217737674713\n",
      "epoch: 346 loss: 0.059803590178489685\n",
      "epoch: 347 loss: 0.05937948077917099\n",
      "epoch: 348 loss: 0.058961108326911926\n",
      "epoch: 349 loss: 0.05854712054133415\n",
      "epoch: 350 loss: 0.05813797935843468\n",
      "epoch: 351 loss: 0.05773376300930977\n",
      "epoch: 352 loss: 0.0573342889547348\n",
      "epoch: 353 loss: 0.05693959444761276\n",
      "epoch: 354 loss: 0.056548915803432465\n",
      "epoch: 355 loss: 0.056162767112255096\n",
      "epoch: 356 loss: 0.05578135699033737\n",
      "epoch: 357 loss: 0.05540384724736214\n",
      "epoch: 358 loss: 0.05503111705183983\n",
      "epoch: 359 loss: 0.054661910980939865\n",
      "epoch: 360 loss: 0.05429689586162567\n",
      "epoch: 361 loss: 0.053936414420604706\n",
      "epoch: 362 loss: 0.05357939004898071\n",
      "epoch: 363 loss: 0.053227152675390244\n",
      "epoch: 364 loss: 0.05287845432758331\n",
      "epoch: 365 loss: 0.052533913403749466\n",
      "epoch: 366 loss: 0.05219319090247154\n",
      "epoch: 367 loss: 0.05185646936297417\n",
      "epoch: 368 loss: 0.05152305215597153\n",
      "epoch: 369 loss: 0.05119338259100914\n",
      "epoch: 370 loss: 0.05086761713027954\n",
      "epoch: 371 loss: 0.050545014441013336\n",
      "epoch: 372 loss: 0.05022601783275604\n",
      "epoch: 373 loss: 0.04991074278950691\n",
      "epoch: 374 loss: 0.049598708748817444\n",
      "epoch: 375 loss: 0.04928943142294884\n",
      "epoch: 376 loss: 0.048983827233314514\n",
      "epoch: 377 loss: 0.04868147894740105\n",
      "epoch: 378 loss: 0.048382468521595\n",
      "epoch: 379 loss: 0.04808637499809265\n",
      "epoch: 380 loss: 0.04779338091611862\n",
      "epoch: 381 loss: 0.04750397801399231\n",
      "epoch: 382 loss: 0.04721682518720627\n",
      "epoch: 383 loss: 0.04693323001265526\n",
      "epoch: 384 loss: 0.046652425080537796\n",
      "epoch: 385 loss: 0.0463743582367897\n",
      "epoch: 386 loss: 0.046099185943603516\n",
      "epoch: 387 loss: 0.04582689702510834\n",
      "epoch: 388 loss: 0.04555744305253029\n",
      "epoch: 389 loss: 0.04529060795903206\n",
      "epoch: 390 loss: 0.045026056468486786\n",
      "epoch: 391 loss: 0.044764723628759384\n",
      "epoch: 392 loss: 0.04450566694140434\n",
      "epoch: 393 loss: 0.044249214231967926\n",
      "epoch: 394 loss: 0.04399560019373894\n",
      "epoch: 395 loss: 0.04374416172504425\n",
      "epoch: 396 loss: 0.04349492862820625\n",
      "epoch: 397 loss: 0.04324847459793091\n",
      "epoch: 398 loss: 0.04300406947731972\n",
      "epoch: 399 loss: 0.04276225343346596\n",
      "epoch: 400 loss: 0.042523134499788284\n",
      "epoch: 401 loss: 0.042286090552806854\n",
      "epoch: 402 loss: 0.0420515313744545\n",
      "epoch: 403 loss: 0.041819021105766296\n",
      "epoch: 404 loss: 0.041588593274354935\n",
      "epoch: 405 loss: 0.04136030748486519\n",
      "epoch: 406 loss: 0.04113440215587616\n",
      "epoch: 407 loss: 0.04091094806790352\n",
      "epoch: 408 loss: 0.04068955034017563\n",
      "epoch: 409 loss: 0.040470048785209656\n",
      "epoch: 410 loss: 0.040252894163131714\n",
      "epoch: 411 loss: 0.04003778100013733\n",
      "epoch: 412 loss: 0.03982473164796829\n",
      "epoch: 413 loss: 0.039613865315914154\n",
      "epoch: 414 loss: 0.03940459340810776\n",
      "epoch: 415 loss: 0.039197102189064026\n",
      "epoch: 416 loss: 0.038991786539554596\n",
      "epoch: 417 loss: 0.038788650184869766\n",
      "epoch: 418 loss: 0.03858736529946327\n",
      "epoch: 419 loss: 0.038387954235076904\n",
      "epoch: 420 loss: 0.038190312683582306\n",
      "epoch: 421 loss: 0.03799432888627052\n",
      "epoch: 422 loss: 0.03780042752623558\n",
      "epoch: 423 loss: 0.037608060985803604\n",
      "epoch: 424 loss: 0.03741757944226265\n",
      "epoch: 425 loss: 0.0372285395860672\n",
      "epoch: 426 loss: 0.037041984498500824\n",
      "epoch: 427 loss: 0.03685630112886429\n",
      "epoch: 428 loss: 0.03667271137237549\n",
      "epoch: 429 loss: 0.0364910364151001\n",
      "epoch: 430 loss: 0.03631053492426872\n",
      "epoch: 431 loss: 0.036131713539361954\n",
      "epoch: 432 loss: 0.035954754799604416\n",
      "epoch: 433 loss: 0.0357789471745491\n",
      "epoch: 434 loss: 0.03560514748096466\n",
      "epoch: 435 loss: 0.03543314337730408\n",
      "epoch: 436 loss: 0.035262297838926315\n",
      "epoch: 437 loss: 0.035092808306217194\n",
      "epoch: 438 loss: 0.034924883395433426\n",
      "epoch: 439 loss: 0.03475847840309143\n",
      "epoch: 440 loss: 0.03459344059228897\n",
      "epoch: 441 loss: 0.03443022072315216\n",
      "epoch: 442 loss: 0.03426840901374817\n",
      "epoch: 443 loss: 0.03410810977220535\n",
      "epoch: 444 loss: 0.03394873067736626\n",
      "epoch: 445 loss: 0.03379076346755028\n",
      "epoch: 446 loss: 0.03363417834043503\n",
      "epoch: 447 loss: 0.03347930684685707\n",
      "epoch: 448 loss: 0.03332550823688507\n",
      "epoch: 449 loss: 0.03317306935787201\n",
      "epoch: 450 loss: 0.033022020012140274\n",
      "epoch: 451 loss: 0.03287209942936897\n",
      "epoch: 452 loss: 0.03272397816181183\n",
      "epoch: 453 loss: 0.03257623687386513\n",
      "epoch: 454 loss: 0.03242999687790871\n",
      "epoch: 455 loss: 0.032285284250974655\n",
      "epoch: 456 loss: 0.032141584903001785\n",
      "epoch: 457 loss: 0.031999215483665466\n",
      "epoch: 458 loss: 0.031858086585998535\n",
      "epoch: 459 loss: 0.03171815350651741\n",
      "epoch: 460 loss: 0.031579528003931046\n",
      "epoch: 461 loss: 0.03144224360585213\n",
      "epoch: 462 loss: 0.031305648386478424\n",
      "epoch: 463 loss: 0.03117002174258232\n",
      "epoch: 464 loss: 0.031035300344228745\n",
      "epoch: 465 loss: 0.030901912599802017\n",
      "epoch: 466 loss: 0.030769633129239082\n",
      "epoch: 467 loss: 0.030638577416539192\n",
      "epoch: 468 loss: 0.030508343130350113\n",
      "epoch: 469 loss: 0.030379418283700943\n",
      "epoch: 470 loss: 0.030251137912273407\n",
      "epoch: 471 loss: 0.030124306678771973\n",
      "epoch: 472 loss: 0.02999846637248993\n",
      "epoch: 473 loss: 0.029873255640268326\n",
      "epoch: 474 loss: 0.029749153181910515\n",
      "epoch: 475 loss: 0.02962622418999672\n",
      "epoch: 476 loss: 0.029504146426916122\n",
      "epoch: 477 loss: 0.029383035376667976\n",
      "epoch: 478 loss: 0.02926308661699295\n",
      "epoch: 479 loss: 0.029143905267119408\n",
      "epoch: 480 loss: 0.02902592346072197\n",
      "epoch: 481 loss: 0.028908884152770042\n",
      "epoch: 482 loss: 0.028792552649974823\n",
      "epoch: 483 loss: 0.028677046298980713\n",
      "epoch: 484 loss: 0.028562599793076515\n",
      "epoch: 485 loss: 0.028449006378650665\n",
      "epoch: 486 loss: 0.028336286544799805\n",
      "epoch: 487 loss: 0.028224365785717964\n",
      "epoch: 488 loss: 0.028113622218370438\n",
      "epoch: 489 loss: 0.028003793209791183\n",
      "epoch: 490 loss: 0.027894699946045876\n",
      "epoch: 491 loss: 0.027786171063780785\n",
      "epoch: 492 loss: 0.02767881751060486\n",
      "epoch: 493 loss: 0.027572182938456535\n",
      "epoch: 494 loss: 0.027466174215078354\n",
      "epoch: 495 loss: 0.027361340820789337\n",
      "epoch: 496 loss: 0.027257215231657028\n",
      "epoch: 497 loss: 0.02715383470058441\n",
      "epoch: 498 loss: 0.027051139622926712\n",
      "epoch: 499 loss: 0.02694910205900669\n",
      "tensor([[ 2.3652,  6.9188,  0.6855,  ..., -5.8243, -5.7542, -5.8057],\n",
      "        [ 3.6491,  5.5685,  0.1863,  ..., -4.1554, -4.1424, -4.2408],\n",
      "        [ 5.1859,  4.1507,  1.6152,  ..., -4.3160, -4.2356, -4.4080],\n",
      "        ...,\n",
      "        [ 3.3629,  1.9210,  4.4061,  ..., -3.5453, -3.3970, -3.6969],\n",
      "        [ 8.9366,  2.8102,  2.0616,  ..., -5.1847, -5.0463, -5.5211],\n",
      "        [ 9.4005,  3.9162,  2.9739,  ..., -6.3282, -6.1281, -6.6168]],\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class RGCN(torch.nn.Module):\n",
    "    def __init__(self, in_c, hid_c, out_c):\n",
    "        super(RGCN,self).__init__()    # 构造函数\n",
    "\n",
    "        self.conv1 = GCNConv(in_channels=in_c, out_channels=hid_c)\n",
    "        self.conv2 = GCNConv(in_channels=hid_c, out_channels=out_c)\n",
    "  \n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index    # x:节点特征矩阵; edge_index:COO格式的图形连接，维度[2,边的数量]，数据：[ [源节点],[目标节点] ]\n",
    "        \n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.conv2(x, edge_index)\n",
    "        \n",
    "        x1 = F.log_softmax(x, dim=1)\n",
    "        \n",
    "        return x1, x\n",
    "\n",
    "model = RGCN(in_c=dataset.num_node_features,hid_c=100,out_c=dataset.num_node_features)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n",
    "\n",
    "print(model(data))\n",
    "\n",
    "model.train()\n",
    "for epoch in range(500):\n",
    "    optimizer.zero_grad()\n",
    "    out1, out = model(data)\n",
    "    # 交叉熵损失\n",
    "    loss = F.nll_loss(out1[data.train_mask], data.y[data.train_mask])       # 负对数似然。\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print('epoch:',epoch, 'loss:',loss.item())  \n",
    "\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c75c3672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calinski-Harabaz Index： 37.447999736749445\n",
      "Silhouette Coefficient： 0.45621094\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn import cluster\n",
    "from sklearn import metrics\n",
    "\n",
    "out=out.cpu().detach().numpy()\n",
    "model = cluster.KMeans(n_clusters=4)\n",
    "model.fit(out)\n",
    "\n",
    "y_predict = model.predict(out)\n",
    "\n",
    "# Calinski-Harabaz Index：越大越好\n",
    "# Silhouette Coefficient：轮廓系数（越大越好)\n",
    "print('Calinski-Harabaz Index：',metrics.calinski_harabasz_score(out,y_predict))\n",
    "print('Silhouette Coefficient：',metrics.silhouette_score(out,y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5ebb415",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>87.243324</td>\n",
       "      <td>-29.089815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>44.904957</td>\n",
       "      <td>-44.577423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>58.422234</td>\n",
       "      <td>-26.015520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>41.585373</td>\n",
       "      <td>-8.468280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>47.248684</td>\n",
       "      <td>17.825443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>45.481873</td>\n",
       "      <td>58.580647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>69.818039</td>\n",
       "      <td>10.933814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-15.511182</td>\n",
       "      <td>-28.528765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-33.722057</td>\n",
       "      <td>-17.421339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-12.428071</td>\n",
       "      <td>38.585865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34.330193</td>\n",
       "      <td>37.279503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14.081661</td>\n",
       "      <td>53.907082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-9.261217</td>\n",
       "      <td>61.959408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.949726</td>\n",
       "      <td>6.391078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-3.178274</td>\n",
       "      <td>-12.211464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-40.102062</td>\n",
       "      <td>4.881661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>61.990547</td>\n",
       "      <td>38.008919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-54.127670</td>\n",
       "      <td>28.308392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-19.868259</td>\n",
       "      <td>-0.782472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24.569275</td>\n",
       "      <td>13.606612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-54.550472</td>\n",
       "      <td>-27.019894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.370134</td>\n",
       "      <td>30.939856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-35.638008</td>\n",
       "      <td>47.966320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>26.281614</td>\n",
       "      <td>-29.351311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.532722</td>\n",
       "      <td>-34.887135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17.401592</td>\n",
       "      <td>-55.412689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-61.194115</td>\n",
       "      <td>-0.573287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-9.605900</td>\n",
       "      <td>16.969772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18.219082</td>\n",
       "      <td>-9.689308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-31.849951</td>\n",
       "      <td>-45.091496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-29.887571</td>\n",
       "      <td>23.422663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-7.069553</td>\n",
       "      <td>-56.377308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>71.201248</td>\n",
       "      <td>-52.318104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56.134090</td>\n",
       "      <td>-73.909271</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0          1\n",
       "0  87.243324 -29.089815\n",
       "0  44.904957 -44.577423\n",
       "0  58.422234 -26.015520\n",
       "3  41.585373  -8.468280\n",
       "2  47.248684  17.825443\n",
       "2  45.481873  58.580647\n",
       "2  69.818039  10.933814\n",
       "1 -15.511182 -28.528765\n",
       "1 -33.722057 -17.421339\n",
       "1 -12.428071  38.585865\n",
       "2  34.330193  37.279503\n",
       "1  14.081661  53.907082\n",
       "1  -9.261217  61.959408\n",
       "1   5.949726   6.391078\n",
       "1  -3.178274 -12.211464\n",
       "1 -40.102062   4.881661\n",
       "2  61.990547  38.008919\n",
       "1 -54.127670  28.308392\n",
       "1 -19.868259  -0.782472\n",
       "1  24.569275  13.606612\n",
       "1 -54.550472 -27.019894\n",
       "1   9.370134  30.939856\n",
       "1 -35.638008  47.966320\n",
       "3  26.281614 -29.351311\n",
       "3   5.532722 -34.887135\n",
       "3  17.401592 -55.412689\n",
       "1 -61.194115  -0.573287\n",
       "3  -9.605900  16.969772\n",
       "1  18.219082  -9.689308\n",
       "3 -31.849951 -45.091496\n",
       "1 -29.887571  23.422663\n",
       "3  -7.069553 -56.377308\n",
       "0  71.201248 -52.318104\n",
       "0  56.134090 -73.909271"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "t_sne = TSNE()\n",
    "data=t_sne.fit_transform(out)\n",
    "data=pd.DataFrame(data)\n",
    "data = pd.DataFrame(data,index=y_predict)\n",
    "data_tsne = pd.DataFrame(t_sne.embedding_, index =y_predict)\n",
    "data_tsne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "902ac67f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f4d2ad9fbe0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXkAAAD4CAYAAAAJmJb0AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAZMElEQVR4nO3df4xd5X3n8fdnPMX2tBMbaodQzHicFmrhzZKwE4s625bEtAEaxfmjqrya3ZK0u6NFLEraSAl0pEr9w1KSVk1dbdt0RKjS7qSUJRRQFDYFN1SqLEzHcSAxHoITY2MXjC0lk5GubTSe7/5xzrUvw8x4xvfce359XtLo3vOce+/5zvH4e5/7nOf5XkUEZmZWTT15B2BmZp3jJG9mVmFO8mZmFeYkb2ZWYU7yZmYV1pt3AK3WrVsXg4ODeYdhZlYq+/fvPx0R6+fbV6gkPzg4yMTERN5hmJmViqSjC+3zcI2ZWYU5yZuZVZiTvJlZhTnJm5lVmJO8mVmFOclb9xwZh8cG4as9ye2R8bwjMqu8Qk2htAo7Mg7PjcD5RrLdOJpsA2wazi8us4pzT9664/nRiwm+6XwjaTezjskkyUtaK+kRSZOSDkn6JUlXSXpK0svp7ZVZHMtKqnFsee1mlomsevK7gf8XEZuBm4BDwH3Anoi4HtiTbltd9Q0sr93MMtH2mLykNcCvAB8HiIg3gTcl7QBuTR/2FeAZ4LPtHs9K6qZdbx2TB1jRl7RbKRybanDw9DRnZmZZ3dvDlnX9DKzpyzssu4QsevKbgFPA30g6IOkBST8NXB0Rr6WPeR24er4nSxqRNCFp4tSpUxmEY4W0aRi2jkHfRkDJ7dYxX3QtiWNTDQ6cnOLMzCwAZ2ZmOXByimNTjUs80/Kmdr/jVdIQ8CzwgYjYJ2k38BPg3ohY2/K4H0XEouPyQ0ND4QJlZsXz5A9OXkjwrVb39nDHz8/bf7MukrQ/Iobm25dFT/44cDwi9qXbjwA3AyclXZMGcA3wRgbHMrMczJfgF2u34mg7yUfE68Crkn4xbdoOvAg8AdyVtt0FPN7uscwsH6t7508VC7VbcWS1GOpeYFzSFcAPgU+QvIE8LOl3gaPAb2V0LDPrsi3r+jlwcorzLaO7K5S0W7FlkuQj4jvAfONB27N4fTPLV3MWjWfXlI/LGpgt1ZHxZIVu41gyv/+mXbWaHTSwps9JvYSc5M2WwrV3rKR81cRsKVx7x0rKSd5sKVx7x0rKSd5sKVx7x0rKSd5sKW7aldTaaeXaO1YCTvJmS+HaO1ZSnl1jtlSbhp3UrXTckzczqzAn+SLyF16bWUY8XFM0XnRjZhlyT75ovOjGzDLkJF80XnRjZhlyki8aL7oxsww5yReNF92YWYac5IvGi27MLEOeXVNEXnRjZhlxT97MrMKc5M3MKsxJ3syswjJL8pJWSDog6evp9iZJ+yQdlvQPkq7I6lhmZrY0WfbkPwkcatn+PPDFiPgF4EfA72Z4LDMzW4JMkrykDcBvAA+k2wI+BDySPuQrwMeyOJYVnIurmRVKVlMo/wz4DNCfbv8s8OOImEm3jwPXzvdESSPACMDAgFd1lpqLq1kXHJtqcPD0NGdmZlnd28OWdf0MrOm79BNrqu2evKSPAG9ExP7LeX5EjEXEUEQMrV+/vt1wLE8urpa7Y1MNnvzBSR596TWe/MFJjk01Lv2kEjk21eDAySnOzMwCcGZmlgMnpyr3e2Ypi578B4CPSroTWAW8A9gNrJXUm/bmNwAnMjiWFZmLq+WqmQDPR7LdTIBAZXq6B09PX/j9ms5H0l6V3zFrbffkI+L+iNgQEYPATuCfI2IY+Bbwm+nD7gIeb/dYVnAurparxRJgVTR78Ettt87Ok/8s8PuSDpOM0X+5g8eyInBxtVzVIQGu7p0/ZS3UbhnXromIZ4Bn0vs/BLZm+fpWcM2Lq8+PJkM0fQNJgvdF165Y3dszb0KvUgLcsq7/LUNSACuUtNv8XKDMsuXiarmpQwJsjrt7ds3SOcmbVURdEuDAmr7K/U6d5CRvViFOgDZXdQbrzMzsbZzkzVyKwSrMwzVWby7FYBXnnrzVm0sxWMU5yVu9uRSDVVy9k7zHYs2lGKzi6pvkm2OxjaNAXByLdaKvF5disIqrb5L3WKxBcnF16xj0bQSU3G4d80VXq4z6zq7xWKw1uRSDVVh9e/IeizWzGqhvkvdYrJnVQH2TvMdizawG6jsmD9UZiz0y7hruZjaveif5KvCyfDNbRH2Ha6rCU0HNbBFO8mXnqaBmtggn+bLzVFAzW0TbSV7SdZK+JelFSQclfTJtv0rSU5JeTm+vbD9cextPBTWzRWTRk58BPh0RNwK3APdIuhG4D9gTEdcDe9Jty5qngprZItqeXRMRrwGvpfenJR0CrgV2ALemD/sK8Azw2XaPZ/OoylRQM8tcplMoJQ0C7wP2AVenbwAArwNXL/CcEWAEYGDA48hmZXFsqsHB09OcmZlldW8PW9b1+0vECyizC6+Sfgb4GvCpiPhJ676ICCDme15EjEXEUEQMrV+/PqtwzKyDjk01OHByijMzswCcmZnlwMkpjk01LvFM67ZMkryknyJJ8OMR8WjafFLSNen+a4A3sjiWldT4OAwOQk9Pcjvuuv1ldvD0NOfndNvOR9JuxZLF7BoBXwYORcSftux6ArgrvX8X8Hi7x7KSGh+HkRE4ehQiktuRESf6Emv24JfabvnJoif/AeC/AR+S9J30507gc8CvSXoZuC3dtjoaHYXGnI/xjUbSbqW0unf+1LFQu+Uni9k1/wpogd3b2319q4BjC6y+XajdCm/Lun4OnJx6y5DNCiXtVix+27XOW2jWlGdTldbAmj7ed/WaCz331b09vO/qNZ5dU0CuQmmdt2tXMgbfOmTT15e0112Jy0QPrOlzUi8B9+St84aHYWwMNm4EKbkdG0va66xZJrpxFIiLZaKP+IK0ZUfJFPZiGBoaiomJibzDMOuOxwbTBD9H30b42CvdjsZKTNL+iBiab5+Ha8zy4jLRuajbSl0P15jlxWWiu66OK3Wd5M3y4jLRXVfHlbpO8mZ5cZnorqvjSl2PyZvlyWWiu2p1b8+8Cb3KK3Wr+5uZmc2xZV0/K+asz6/6Sl335M2sNpqzaOo0u8ZJ3uqhxCtLLVt1W6nrJG/V11xZej6dJtdcWQpO9FZ5HpPvtiPjyUrHr/Ykt17C3nnPj15M8E3nG0m7WcW5J99N7lHmwytLrcbck+8m9yjz4ZWlVmNO8t3kHmU+vLLUasxJvpvco8yHV5ZajXlMvptu2vXWMXlwj7JbvLLUaso9+W5yj9LMuqzjPXlJtwO7gRXAAxHxuU4fs9DcozSzLupoT17SCuAvgDuAG4H/IunGTA/ieedmZgvq9HDNVuBwRPwwIt4EHgJ2ZPbq/o7MzvAbp1lldDrJXwu82rJ9PG27QNKIpAlJE6dOnVreq3veefb8xmlWKblfeI2IsYgYioih9evXL+/JnneePb9xmlVKp5P8CeC6lu0NaVs2PO88e37jNKuUTif5fwOul7RJ0hXATuCJzF7dKxmz5zdOs0rpaJKPiBngfwHfBA4BD0fEwcwO4Hnn2fMbp1mldHyefER8A/hGxw7geefZap5Lf8GGWSW4rIG9nd84zSoj99k1ZmbWOU7yZmYV5iRvZlZhTvJmlhgfh8FB6OlJbse9yrkKnOTNslLmmj/j4zAyAkePQkRyOzLiRF8BTvJmWSh7zZ/RUWjMKWfRaCTtVmpO8mZZKHvNn2MLlK1YqN1Kw0neLAtlr/kzsEDZioXarTSc5K24yjTGXfaaP7t2Qd+cchZ9fUm7lZqTvBVT2ca4y17zZ3gYxsZg40aQktuxsaTdSk0RkXcMFwwNDcXExETeYVgRPDaYJvg5+jbCx17pdjRLc2TcNX8sF5L2R8TQfPtcu8aKqYxj3K75YwXk4RorprKPcZsVhJO8FVPZx7jNCsJJ3orJXwhjlgmPyVtxeYzbrG3uyZdBmeaLm1mhuCdfdM354s0l88354uBerpldknvyRVf2mihmlqu2krykP5Y0KekFSf8oaW3LvvslHZb0kqQPtx1pXZVxvriZFUa7PfmngP8QEf8R+D5wP4CkG4GdwBbgduAvJa1o81j15PniZtaGtpJ8RPxTRMykm88CG9L7O4CHIuJcRBwBDgNb2zlWbXm+uJm1Icsx+d8BnkzvXwu82rLveNr2NpJGJE1Imjh16lSG4VSE54ubWRsuObtG0tPAu+bZNRoRj6ePGQVmgGXP7YuIMWAMkgJly31+LXi+uJldpksm+Yi4bbH9kj4OfATYHhdLWp4Armt52Ia0zczMuqjd2TW3A58BPhoRrfP8ngB2SlopaRNwPfBcO8cyM7Pla3cx1P8GVgJPSQJ4NiL+Z0QclPQw8CLJMM49EXG+zWOZmdkytZXkI+IXFtm3C/AUEDOzHLmsgZkVzuS5Sfae3cv07DT9Pf1sW7WNzSs35x1WKTnJm1mhTJ6bZE9jDzMkS3CmZ6fZ09gD4ER/Gepbu8aVHc0Kae/ZvRcSfNMMM+w9uzeniMqtnj15V3Y0K6zp2elltdvi6tmTd2XH9vmTkHVIf0//stptcfVM8q7s2J7mJ6HGUSAufhJyom/f+DgMDkJPT3I7Xr9zum3VNnrnDDL00su2VdtyiqizJs9N8uDUg+z+0W4enHqQyXOTmb5+PZO8Kzu2x5+EOmN8HEZG4OhRiEhuR0Zql+g3r9zM9r7tF3ru/T39bO/bXsmLrs2LzM2hqOZF5iwTfT3H5G/a9dYxeXBlx+XwJ6HOGB2Fxpw3z0YjaR+u17WizSs3VzKpz7XYReasfv969uRd2bE9/iTUGccWeJNcqN1KrxsXmevZkwdXdmyHPwl1xsBAMkQzX7tVUn9P/7wJPcuLzPXsyVt7/EmoM3btgr45XxDT15e0WyV14yJzfXvy1h5/Espec9x9dDQZohkYSBJ8zcbj66Q57t7JEg66WAI+f0NDQzExMZF3GGZmpSJpf0QMzbfPwzVWfZ57bjXm4Rqrtubc8+bUxObcc/AwiNWCe/JWbYvNPTerASd5qzbPPbeac5K3altojrnnnltNOMlbtXnuudWck7xV2/AwjI3Bxo0gJbdjY77oarWRSZKX9GlJIWldui1Jfy7psKQXJN2cxXHMLsvwMLzyCszOJrdO8FYjbSd5SdcBvw60Xsm6A7g+/RkB/qrd45iZ2fJl0ZP/IvAZoHXp7A7gbyPxLLBW0jUZHMvMzJahrSQvaQdwIiKen7PrWuDVlu3jaZuZmXXRJVe8SnoaeNc8u0aBPyAZqrlskkZIhnQY8LQ2M7NMXTLJR8Rt87VLeg+wCXheEsAG4NuStgIngOtaHr4hbZvv9ceAMUgKlC0neDMzW9xlD9dExHcj4p0RMRgRgyRDMjdHxOvAE8Bvp7NsbgGmIuK1bEI2M7Ol6lSBsm8AdwKHgQbwiQ4dx8y6YPLcZEdrnlvnZLYYKu3Rn07vR0TcExE/HxHviQgXic+SS+daF02em2RPY8+Fr6mbnp1mT2MPk+cmc47MlsIrXsumWTr36FGIuFg614neOmTv2b3MMPOWthlm2Ht2b04R2XI4yZeNS+dal833RdOLtVuxOMmXjUvnWpf19/Qvq92KxUm+bFw617ps26pt9M6Zo9FLL9tWbcspIlsOJ/mycelc67LNKzezvW/7hZ57f08/2/u2e3ZNSfg7XsumWUFxdDQZohkYSBK8KytaB21eudlJvaSc5MtoeNhJ3cyWxMM1ZmYV5iRvZlZhTvJmZhXmMXkzq6061ORxkjezWmrW5GmWbGjW5AEqleg9XGNmtVSXmjxO8mZWS3WpyeMkb5YFl38unbrU5HGSt/xUJTG6/HMp1aUmj5O85aNKidHln0upLjV5FFGc784eGhqKiQl/iVQtDA4miX2ujRvhlVe6HU17enqSN6q5JJid7X48VjuS9kfE0Hz73JO3fFSpLr7LP1uBOclbPqqUGF3+2QrMSd7yUaXEODwMY2PJUJOU3I6NuVKoFULbSV7SvZImJR2U9IWW9vslHZb0kqQPt3scq5iqJcbh4eRawuxsclvW38Mqp62yBpI+COwAboqIc5LembbfCOwEtgA/Bzwt6YaION9uwFYhrotv1nHt9uTvBj4XEecAIuKNtH0H8FBEnIuII8BhYGubxzIzs2VqN8nfAPyypH2S/kXS+9P2a4FXWx53PG17G0kjkiYkTZw6darNcMzMrNUlh2skPQ28a55do+nzrwJuAd4PPCzp3csJICLGgDFI5skv57lmZra4Syb5iLhtoX2S7gYejWRF1XOSZoF1wAngupaHbkjbzMysi9odrnkM+CCApBuAK4DTwBPATkkrJW0Crgeea/NYZma2TO1+aciDwIOSvge8CdyV9uoPSnoYeBGYAe7xzBozs+5rK8lHxJvAf11g3y6ghCtbzMyqwytezcwqzEnezKzCnOTNzCrMSd7MrMKc5M3MKsxJ3syswpzkzcwqzEnezKzCnOTNzCrMSd7MrMKc5M3MKqzdAmVmZm2ZPDfJ3rN7mZ6dpr+nn22rtrF55ea8w6oMJ3kzy83kuUn2NPYwwwwA07PT7GnsAXCiz4iHa8wsN3vP7r2Q4JtmmGHv2b05RVQ9TvJmlpvp2elltdvyOcmbWW76e/qX1W7L5yRvZrnZtmobvXMuDfbSy7ZV23KKqHp84dXMctO8uOrZNZ3jJG9mudq8crOTegd5uMbMrMLaSvKS3ivpWUnfkTQhaWvaLkl/LumwpBck3ZxNuGZmthzt9uS/APxRRLwX+MN0G+AO4Pr0ZwT4qzaPY2Zml6HdJB/AO9L7a4B/T+/vAP42Es8CayVd0+axzMxsmdq98Pop4JuS/oTkDaM57+la4NWWxx1P216b+wKSRkh6+wwMDLQZjpmZtbpkkpf0NPCueXaNAtuB34uIr0n6LeDLwG3LCSAixoCx9FinJB1dzvMXsA44ncHrdJJjzIZjzIZjzEZeMW5caIci4rJfVdIUsDYiQpKAqYh4h6S/Bp6JiL9PH/cScGtEvK0n3wmSJiJiqBvHulyOMRuOMRuOMRtFjLHdMfl/B341vf8h4OX0/hPAb6ezbG4hSf5dSfBmZnZRu2Py/wPYLakXOEs6tg58A7gTOAw0gE+0eRwzM7sMbSX5iPhX4D/N0x7APe28dpvGcjz2UjnGbDjGbDjGbBQuxrbG5M3MrNhc1sDMrMKc5M3MKqxSSV7SvZImJR2U9IWW9vvTOjovSfpwnjE2Sfq0pJC0Lt0uRL0fSX+cnsMXJP2jpLUt+wpzHiXdnsZxWNJ9ecbSJOk6Sd+S9GL6N/jJtP0qSU9Jejm9vbIAsa6QdEDS19PtTZL2pefzHyRdkXN8ayU9kv4tHpL0S0U7j5J+L/13/p6kv5e0qmjnEYCIqMQP8EHgaWBluv3O9PZG4HlgJbAJ+AGwIudYrwO+CRwF1qVtdwJPAgJuAfblFNuvA73p/c8Dny/aeQRWpMd/N3BFGteNBfgbvAa4Ob3fD3w/PW9fAO5L2+9rntOcY/194KvA19Pth4Gd6f0vAXfnHN9XgP+e3r8CWFuk80iygv8IsLrl/H28aOcxIirVk78b+FxEnAOIiDfS9h3AQxFxLiKOkEzr3JpTjE1fBD5DUvunqRD1fiLinyKi+c3KzwIbWuIrynncChyOiB9GxJvAQ2l8uYqI1yLi2+n9aeAQSTLYQZK0SG8/lkuAKUkbgN8AHki3RbLO5ZH0IbnGKGkN8CskK+iJiDcj4scU7DySzE5cnU4h7yMp21KY89hUpSR/A/DL6Uelf5H0/rR9oTo6uZC0AzgREc/P2VWoOFO/Q/LpAooVX5FimZekQeB9wD7g6ri4GPB14Oq84kr9GUknYzbd/lngxy1v7nmfz03AKeBv0iGlByT9NAU6jxFxAvgT4BhJcp8C9lOs8wiU7JuhLlFHpxe4imSo4/3Aw5Le3cXwLrhEnH9AMiSSm8Xii4jH08eMAjPAeDdjqwJJPwN8DfhURPwk6SgnIiIk5TZvWdJHgDciYr+kW/OK4xJ6gZuBeyNin6TdJMMzFxTgPF5J8sliE/Bj4P8Ct+cVz2JKleQjYsHiZ5LuBh6NZDDsOUmzJMWCTpCMgTdtSNu6Hqek95D8UTyf/sffAHxbyZetdC3Oxc5jGufHgY8A29PzSTfjW4IixfIWkn6KJMGPR8SjafNJSddExGvpENwbC79Cx30A+KikO4FVJKXCd5MMD/amvdC8z+dx4HhE7Eu3HyFJ8kU6j7cBRyLiFICkR0nObZHOI1Ct4ZrHSC6+IukGkos1p0nq6OyUtFLSJpIvMnkujwAj4rsR8c6IGIyIQZI/5psj4nUKUu9H0u0kH+U/GhGNll2FOY/AvwHXpzMZrgB2pvHlKh3b/jJwKCL+tGXXE8Bd6f27gMe7HVtTRNwfERvSv7+dwD9HxDDwLeA304flHePrwKuSfjFt2g68SIHOI8kwzS2S+tJ/92aMhTmPF+R95TerH5Kk/n+A7wHfBj7Usm+UZDbGS8AdecfaEtcrXJxdI+Av0ji/CwzlFNNhkvHu76Q/XyrieSSZjfT9NJ7RvP8t05j+M8nF9Bdazt+dJGPee0gK+D0NXJV3rGm8t3Jxds27Sd60D5MMPazMObb3AhPpuXwMuLJo5xH4I2AyzTl/RzLzrFDnMSJc1sDMrMqqNFxjZmZzOMmbmVWYk7yZWYU5yZuZVZiTvJlZhTnJm5lVmJO8mVmF/X+AirnkZvzVuwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "d = data_tsne[data_tsne.index == 0]     #找出聚类类别为0的数据对应的降维结果\n",
    "plt.scatter(d[0], d[1],c='lightgreen',marker='o')\n",
    "d = data_tsne[data_tsne.index == 1]\n",
    "plt.scatter(d[0], d[1], c='orange',\tmarker='o')\n",
    "d = data_tsne[data_tsne.index == 2]\n",
    "plt.scatter(d[0], d[1], c='lightblue',marker='o')\n",
    "d = data_tsne[data_tsne.index == 3]\n",
    "plt.scatter(d[0], d[1], c='red',marker='o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be35055",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
